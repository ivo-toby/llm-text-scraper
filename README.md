# ğŸš€ Universal Documentation Scraper

This script scrapes structured documentation from any website, processes it for AI training, and outputs a well-organized `llms-full.txt` file. It uses **Selenium** to dynamically extract links and **OpenAI GPT-4o-mini** to clean and format the content.

## ğŸ›  Features

âœ… **Scrapes Any Documentation Site** â€“ Just provide a base URL and filter path.  
âœ… **Uses JavaScript Rendering** â€“ Extracts dynamically loaded content with Selenium.  
âœ… **AI-Powered Processing** â€“ Uses GPT-4o-mini to summarize and format content.  
âœ… **Efficient Caching** â€“ Saves URLs and page content to `tmp/` for reuse.  
âœ… **Outputs Well-Structured Documentation** â€“ Clear section titles, clean formatting, and code snippets.  
âœ… **Supports Custom Content Selectors** â€“ Specify a CSS selector to precisely extract documentation content.

See the `scraped` folder for SDK's that have already been generated by this script, feel free to create a PR to add your scraped SDK's

---

## ğŸ”§ Installation

### **1ï¸âƒ£ Install Prerequisites**

Ensure you have Python 3.8+ installed.

#### Install dependencies:

```bash
pip install selenium webdriver-manager beautifulsoup4 openai chromedriver-autoinstaller
```

#### Set up OpenAI API Key (required for AI processing):

```bash
export OPENAI_API_KEY="your-api-key"
```

Replace `your-api-key` with your actual OpenAI API key.

---

## ğŸš€ Usage

The script requires two main parameters and an optional one:

- `--base-url` â†’ The website's root documentation URL.
- `--filter-path` â†’ (Optional) The section of the documentation to scrape.
- `--custom-selector` â†’ (Optional) A CSS-style selector for extracting specific content.

### **Basic Usage**

```bash
python scraper.py --base-url https://mastra.ai --filter-path /docs/reference/
```

This will scrape Mastra.ai documentation, filter pages under `/docs/reference/`, and save them in `llms-full.txt`.

### **Scrape Other Sites**

#### OpenAI API Reference:

```bash
python scraper.py --base-url https://platform.openai.com --filter-path /docs/api-reference/
```

#### Vercel AI SDK:

```bash
python scraper.py --base-url https://sdk.vercel.ai --filter-path /docs/
```

### **Using a Custom CSS Selector**

If the default extraction method does not work, you can specify a custom CSS selector.

#### Example: Extracting Content from a Specific `<div>`

```bash
python scraper.py --base-url https://example.com/docs --custom-selector "div.main-doc-container"
```

#### Example: Multiple CSS Selectors

```bash
python scraper.py --base-url https://example.com/docs --custom-selector "article, section.content, div.doc-content"
```

This allows the scraper to precisely target the documentation content.

#### Example: Using a predefined list or URL's

If you have a list of URL's that you want to scrape instead of having the script determine the list, you can convert the text format to pickle format by using the pickly.py conversion script.

```bash

python pickly.py --source urls.txt --output ./tmp/urls_cache.pkl
```

Make sure you move the pickly file to ./tmp/urls_cache.pkl, the scraper will pick that file up and start processing the links.

---

## ğŸ—‚ Caching & Refreshing

The script stores scraped URLs and content in the `tmp/` folder to avoid redundant fetching.

- **To force a full refresh**, delete the `tmp/` folder before running the script:

```bash
rm -rf tmp/
```

---

## ğŸ“ Output Format

The extracted content is saved in `llms-full.txt` with a clean structure:

```plaintext
# Documentation from https://mastra.ai
> Extracted content from /docs/reference/

----------------------------------------
Client SDK - JS Overview
https://mastra.ai/docs/reference/client-js
----------------------------------------

(Mastra Client SDK formatted details)

----------------------------------------
RAG Embeddings
https://mastra.ai/docs/reference/rag/embeddings
----------------------------------------

(Formatted RAG documentation)
```

---

## ğŸ›  Troubleshooting

- **No URLs found?** The site may block automated requests. Try increasing `time.sleep(3)` to `time.sleep(5)` in `fetch_page()`.
- **Content not loading?** Ensure JavaScript-rendered pages are properly fetched by Selenium.
- **Selenium WebDriver Errors?** Run:
  ```bash
  pip install --upgrade webdriver-manager
  ```

---

## ğŸ“œ License

This project is open-source and can be used freely.
