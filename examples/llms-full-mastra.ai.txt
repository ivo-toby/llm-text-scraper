# Mastra.ai SDK Documentation
> Comprehensive reference for the Mastra.ai SDK.

----------------------------------------
Client Js
https://mastra.ai/docs/reference/client-js
----------------------------------------

# Mastra Client SDK - JS Overview

The Mastra Client SDK provides a type-safe interface for interacting with Mastra REST APIs in TypeScript and JavaScript applications. It supports various features including agents, vectors, memory, tools, and workflows.

## Installation

To install the SDK, use one of the following commands:

```bash
npm install @mastra/client-js
```
```bash
yarn add @mastra/client-js
```
```bash
pnpm add @mastra/client-js
```

### Requirements

- Node.js 16.x or later
- TypeScript 4.7+ (for TypeScript users)
- Modern browser environment with Fetch API support (for browser usage)

## Local Development

To point the client to your local Mastra server:

```javascript
const client = new MastraClient({
  baseUrl: "http://localhost:4111" // Default Mastra server port
});
```

## Basic Configuration

Minimal configuration requires only the `baseUrl`:

```javascript
import { MastraClient } from "@mastra/client-js";

const client = new MastraClient({
  baseUrl: "http://localhost:4111"
});
```

### Configuration Options

Here’s a complete example with all configuration options:

```javascript
const client = new MastraClient({
  baseUrl: "http://localhost:4111", // Required
  retries: 3,                        // Optional: Number of retry attempts (default: 3)
  backoffMs: 300,                    // Optional: Initial backoff time in ms (default: 300)
  maxBackoffMs: 5000,                // Optional: Maximum backoff time in ms (default: 5000)
  headers: {                         // Optional: Custom headers
    "Custom-Header": "value"
  }
});
```

| Option         | Type                       | Default | Description                                      |
|----------------|----------------------------|---------|--------------------------------------------------|
| `baseUrl`      | string                     | -       | The base URL of your Mastra API endpoint (required) |
| `retries`      | number                     | 3       | Number of times to retry failed requests         |
| `backoffMs`    | number                     | 300     | Initial backoff time in milliseconds             |
| `maxBackoffMs` | number                     | 5000    | Maximum backoff time in milliseconds             |
| `headers`      | Record<string, string>     | {}      | Custom headers to include in all requests        |

## Available Resources

The client provides access to the following resources:

- **Agents**: Create and manage AI agents, generate responses, and handle streaming interactions.
- **Memory**: Manage conversation threads and message history.
- **Tools**: Access and execute tools available to agents.
- **Workflows**: Create and manage automated workflows.
- **Vectors**: Handle vector operations for semantic search and similarity matching.

## Quick Example

Here’s a simple example of using the client with an agent:

```javascript
const client = new MastraClient({
  baseUrl: "http://localhost:4111"
});

// Get an agent instance
const agent = client.getAgent("your-agent-id");

// Generate a response
const response = await agent.generate({
  messages: [
    {
      role: "user",
      content: "Hello!"
    }
  ]
});
```

Last updated on February 20, 2025.

----------------------------------------
Agents/Createtool
https://mastra.ai/docs/reference/agents/createTool
----------------------------------------

### Summary of `createTool()` API

The `createTool()` function is used to define a tool that agents or workflows can execute. Each tool is characterized by its schema, executor function, and integration access. 

#### Key Components:
- **Tool Schema**: Defines the inputs for the tool.
- **Executor Function**: Implements the tool's logic and handles the execution.
- **Integration Access**: Provides access to configured integrations.

### Example Tool: `stockPrices`

This example demonstrates how to create a tool that fetches the last day's closing stock price for a given symbol.

#### Code Implementation:

```typescript
import { createTool } from "@mastra/core/logger";
import { z } from "zod";

const getStockPrice = async (symbol: string) => {
    const data = await fetch(`https://mastra-stock-data.vercel.app/api/stock-data?symbol=${symbol}`)
        .then(r => r.json());
    return data.prices["4. close"];
};

export const stockPrices = createTool({
    id: "Get Stock Price",
    inputSchema: z.object({
        symbol: z.string(),
    }),
    description: `Fetches the last day's closing stock price for a given symbol`,
    execute: async ({ context }) => {
        console.log("Using tool to fetch stock price for", context.symbol);
        return {
            symbol: context.symbol,
            currentPrice: await getStockPrice(context.symbol),
        };
    },
});
```

### API Signature

#### Parameters:
- **label**: `string` - Name of the tool (e.g., "Get Stock Prices").
- **schema**: `ZodSchema` - Zod schema for validating inputs.
- **description**: `string` - Explanation of the tool's functionality.
- **executor**: `(params: ExecutorParams) => Promise<any>` - Async function that executes the tool's logic.

#### ExecutorParams:
- **data**: `object` - Validated input data (e.g., symbol).
- **integrationsRegistry**: `function` - Function to access connected integrations.
- **runId?**: `string` - Optional run ID for the current execution.
- **agents**: `Map<string, Agent<any>>` - Registered agents.
- **engine?**: `MastraEngine` - Instance of the Mastra engine.
- **llm**: `LLM` - Instance of the LLM.
- **outputSchema?**: `ZodSchema` - Schema for validating outputs.

#### Returns:
- **ToolApi**: `object` - Contains the schema, label, description, and executor function.
- **ToolApi.schema**: `ZodSchema<IN>` - Schema for validating inputs.
- **label**: `string` - Name of the tool.
- **description**: `string` - Description of the tool's functionality.
- **outputSchema?**: `ZodSchema<OUT>` - Schema for validating outputs.
- **executor**: `(params: IntegrationApiExecutorParams<IN>) => Promise<OUT>` - Executes the tool's logic.

### Last Updated
- February 20, 2025

----------------------------------------
Agents/Generate
https://mastra.ai/docs/reference/agents/generate
----------------------------------------

# Agent API Documentation

## Method: `generate()`

The `generate()` method interacts with an agent to produce text or structured responses. It accepts two parameters: `messages` and an optional `options` object.

### Parameters

1. **messages**: 
   - Type: `string | Array<string> | Array<Message>`
   - Description: The input messages for the agent. It can be:
     - A single string
     - An array of strings
     - An array of message objects with `role` and `content` properties.

   - **Message Object Structure**:
     ```typescript
     interface Message {
       role: 'system' | 'user' | 'assistant';
       content: string;
     }
     ```

2. **options** (Optional):
   - Type: `object`
   - Description: Additional options for the `generate` method, which can include:
     - `structuredOutput` (or `schema`): Defines the expected structure of the output (JSON Schema or Zod schema).
     - Other options such as `onStepFinish`, `maxSteps`, `threadId`, `resourceId`, etc.

### Returns

The return value of the `generate()` method varies based on the `options` provided:

- **text** (optional): `string` - The generated text response if `structuredOutput` is not provided.
- **object** (optional): `object` - The structured response based on the provided schema if `structuredOutput` is specified.
- **toolCalls** (optional): `Array<ToolCall>` - The tool calls made during the generation process.
- **error** (optional): `string` - An error message if the generation fails.

### ToolCall Structure

- **toolName**: `string` - The name of the invoked tool.
- **args**: `any` - The arguments passed to the tool.

### Related Methods

- For real-time streaming responses, refer to the `stream()` method documentation.

**Last updated on February 20, 2025**

----------------------------------------
Agents/Getagent
https://mastra.ai/docs/reference/agents/getAgent
----------------------------------------

### API Reference: Agents

#### Method: `getAgent()`

**Description:**  
Retrieves an agent based on the provided configuration.

**Signature:**
```javascript
async function getAgent({
  connectionId,
  agent,
  apis,
  logger,
}: {
  connectionId: string;
  agent: Record<string, any>;
  apis: Record<string, IntegrationApi>;
  logger: any;
}): Promise<(props: { prompt: string }) => Promise<any>> {
  return async (props: { prompt: string }) => {
    return { message: "Hello, world!" };
  };
}
```

**Parameters:**
- `connectionId` (string): The connection ID for the agent's API calls.
- `agent` (Record<string, any>): The agent configuration object.
- `apis` (Record<string, IntegrationAPI>): A map of API names to their respective API objects.
- `logger` (any): Logger instance for logging purposes.

**Returns:**  
A promise that resolves to a function, which takes an object with a `prompt` string and returns a promise resolving to any type.

**Last Updated:** February 20, 2025

----------------------------------------
Agents/Stream
https://mastra.ai/docs/reference/agents/stream
----------------------------------------

### stream() Method Overview

The `stream()` method facilitates real-time streaming of responses from an agent. It accepts `messages` and an optional `options` object as parameters, similar to the `generate()` method.

#### Parameters

1. **messages**: Can be one of the following:
   - A single string
   - An array of strings
   - An array of message objects with `role` and `content` properties

   **Message Object Structure**:
   ```typescript
   interface Message {
       role: 'system' | 'user' | 'assistant';
       content: string;
   }
   ```

2. **options** (Optional): An object that may include:
   - `output?`: `string | JSONSchema7 | ZodSchema` - Defines output format ("text" or a schema).
   - `context?`: `CoreMessage[]` - Additional context messages for the agent.
   - `threadId?`: `string` - Identifier for maintaining conversation context.
   - `resourceId?`: `string` - Identifier for the user/resource interacting with the agent.
   - `onFinish?`: `(result: string) => Promise<void> | void` - Callback when streaming is complete.
   - `onStepFinish?`: `(step: string) => void` - Callback after each step during streaming.
   - `maxSteps?`: `number` - Maximum allowed steps during streaming.
   - `toolsets?`: `ToolsetsInput` - Additional toolsets for the agent.
   - `temperature?`: `number` - Controls randomness in output (0.2 for deterministic, 0.8 for random).

#### Returns

The method returns a promise resolving to an object that may contain:
- `textStream?`: `AsyncIterable<string>` - Stream of text chunks (when output is "text").
- `objectStream?`: `AsyncIterable<object>` - Stream of structured data (when a schema is provided).
- `object?`: `Promise<object>` - Final structured output when using a schema.

#### Examples

1. **Basic Text Streaming**:
   ```javascript
   const stream = await myAgent.stream([{ role: 'user', content: 'Tell me a story.' }]);

   for await (const chunk of stream.textStream) {
       process.stdout.write(chunk);
   }
   ```

2. **Structured Output Streaming with Thread Context**:
   ```javascript
   const schema = {
       type: 'object',
       properties: {
           summary: { type: 'string' },
           nextSteps: { type: 'array', items: { type: 'string' } }
       },
       required: ['summary', 'nextSteps']
   };

   const response = await myAgent.stream("What should we do next?", {
       output: schema,
       threadId: "project-123",
       onFinish: text => console.log("Finished:", text)
   });

   for await (const chunk of response.textStream) {
       console.log(chunk);
   }

   const result = await response.object;
   console.log("Final structured result:", result);
   ```

### Key Differences
The `stream()` method of Agents differs from LLM's `stream()` in that Agents maintain conversation context through `threadId`, can access tools, and integrate with the agent's memory system.

----------------------------------------
Cli/Build
https://mastra.ai/docs/reference/cli/build
----------------------------------------

# Mastra CLI Documentation Summary

## Command: `mastra build`

The `mastra build` command compiles your Mastra project into a production-ready Hono server, which is a lightweight web framework ideal for deploying Mastra agents as HTTP endpoints.

### Usage
```bash
mastra build [options]
```

### Options
- `--dir <path>`: Specifies the directory containing your Mastra project (default is the current directory).

### Functionality
- Locates the Mastra entry file (`src/mastra/index.ts` or `src/mastra/index.js`).
- Creates a `.mastra` output directory.
- Bundles code using Rollup with:
  - Tree shaking for optimal bundle size.
  - Node.js environment targeting.
  - Source map generation for debugging.

### Example Commands
- Build from the current directory:
  ```bash
  mastra build
  ```
- Build from a specific directory:
  ```bash
  mastra build --dir ./my-mastra-project
  ```

### Output
The command generates a production bundle in the `.mastra` directory, which includes:
- A Hono-based HTTP server with Mastra agents exposed as endpoints.
- Bundled JavaScript files optimized for production.
- Source maps for debugging.

### Use Cases
The output is suitable for:
- Deploying to cloud servers (e.g., EC2, Digital Ocean).
- Running in containerized environments.
- Using with container orchestration systems.

**Last updated on February 20, 2025.**

----------------------------------------
Cli/Deploy
https://mastra.ai/docs/reference/cli/deploy
----------------------------------------

# Mastra CLI Deployment Commands

## Overview
The Mastra CLI provides commands to deploy projects to various platforms. Below are the available deployment commands along with their parameters.

## Deployment Commands

### 1. Deploy to Vercel
```bash
mastra deploy vercel
```
- **Description**: Deploys your Mastra project to Vercel.

### 2. Deploy to Cloudflare
```bash
mastra deploy cloudflare
```
- **Description**: Deploys your Mastra project to Cloudflare.

### 3. Deploy to Netlify
```bash
mastra deploy netlify
```
- **Description**: Deploys your Mastra project to Netlify.

## Common Flags

### -d, --dir <dir>
- **Description**: Specifies the path to your Mastra project folder.

## Last Updated
- February 20, 2025

This documentation provides a concise reference for deploying Mastra projects using the CLI.

----------------------------------------
Cli/Dev
https://mastra.ai/docs/reference/cli/dev
----------------------------------------

# mastra dev Command Documentation

The `mastra dev` command initiates a development server that provides REST endpoints for agents, tools, and workflows.

## Parameters

- `--dir`: *string*  
  Specifies the path to the Mastra folder (containing agents, tools, and workflows). Defaults to the current working directory.

- `--env`: *string*  
  Specifies which environment file to load. Defaults to `.env.development`, falling back to `.env` if not found.

- `--tools`: *string*  
  Comma-separated paths to additional tool directories to register (e.g., `src/tools/dbTools,src/tools/scraperTools`).

- `--port`: *number*  
  Specifies the port number for the development server. Defaults to `4111`.

## REST Endpoints

### Agent Routes
Agents should be exported from `src/mastra/agents`.

- **GET /api/agents**  
  Lists the registered agents in your Mastra folder.

- **POST /api/agents/:agentId/generate**  
  Sends a text-based prompt to the specified agent and returns the agent’s response.

### Tool Routes
Tools should be exported from `src/mastra/tools` or the configured tools directory.

- **POST /api/tools/:toolName**  
  Invokes a specific tool by name, passing input data in the request body.

### Workflow Routes
Workflows should be exported from `src/mastra/workflows` or the configured workflows directory.

- **POST /api/workflows/:workflowName/start**  
  Starts the specified workflow.

- **POST /api/workflows/:workflowName/:instanceId/event**  
  Sends an event or trigger signal to an existing workflow instance.

- **GET /api/workflows/:workflowName/:instanceId/status**  
  Returns status information for a running workflow instance.

### OpenAPI Specification
- **GET /openapi.json**  
  Returns an auto-generated OpenAPI specification for your project’s endpoints.

## Additional Notes
- The default port is `4111`.
- Ensure environment variables are set up in your `.env.development` or `.env` file for any providers used (e.g., `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`).

## Example Request
To test an agent after running `mastra dev`:

```bash
curl -X POST http://localhost:4111/api/agents/myAgent/generate \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      { "role": "user", "content": "Hello, how can you assist me today?" }
    ]
  }'
```

## Related Documentation
- **REST Endpoints Overview**: Detailed usage of the dev server and agent endpoints.
- **mastra deploy**: Instructions for deploying your project to Vercel or Cloudflare.

_Last updated on February 20, 2025_

----------------------------------------
Cli/Init
https://mastra.ai/docs/reference/cli/init
----------------------------------------

# Mastra CLI Documentation

## Command: `mastra init`

The `mastra init` command initializes a new Mastra project. It can be executed in three modes:

### 1. Interactive Mode (Recommended)
- Run `mastra init` without flags.
- Prompts include:
  - Directory selection for Mastra files.
  - Component selection (Agents, Tools, Workflows).
  - Default LLM provider choice (OpenAI, Anthropic, Groq).
  - Option to include example code.

### 2. Quick Start with Defaults
- Command: 
  ```bash
  mastra init --default
  ```
- Default setup includes:
  - Source directory: `src/`
  - All components: agents, tools, workflows.
  - Default LLM provider: OpenAI.
  - No example code included.

### 3. Custom Setup
- Command:
  ```bash
  mastra init --dir <directory> --components <components> --llm <provider> --example
  ```
- Options:
  - `-d, --dir`: Specify directory for Mastra files (default: `src/mastra`).
  - `-c, --components`: Comma-separated list of components (options: agents, tools, workflows).
  - `-l, --llm`: Default LLM provider (options: openai, anthropic, groq).
  - `-k, --llm-api-key`: API key for the selected LLM provider (added to `.env` file).
  - `-e, --example`: Include example code.
  - `-ne, --no-example`: Skip example code.

### Last Updated
- February 20, 2025.

----------------------------------------
Client Js/Agents
https://mastra.ai/docs/reference/client-js/agents
----------------------------------------

# Agents API Documentation Summary

The Agents API allows interaction with Mastra AI agents for generating responses, streaming interactions, and managing agent tools.

## Key API Methods

### 1. Get All Agents
Retrieve a list of all available agents.
```javascript
const agents = await client.getAgents();
```

### 2. Get Specific Agent
Obtain an instance of a specific agent using its ID.
```javascript
const agent = client.getAgent("agent-id");
```

### 3. Get Agent Details
Retrieve detailed information about an agent.
```javascript
const details = await agent.details();
```

### 4. Generate Response
Generate a response from the agent based on user input.
```javascript
const response = await agent.generate({
  messages: [{
    role: "user",
    content: "Hello, how are you?",
  }],
  threadId: "thread-1", // Optional: Thread ID for context
  resourceid: "resource-1", // Optional: Resource ID
  output: {}, // Optional: Output configuration
});
```

### 5. Stream Response
Stream a response from the agent for real-time interactions.
```javascript
const response = await agent.stream({
  messages: [{
    role: "user",
    content: "Tell me a story",
  }],
});

// Read from response body
const reader = response.body.getReader();
while (true) {
  const { done, value } = await reader.read();
  if (done) break;
  console.log(new TextDecoder().decode(value));
}
```

### 6. Get Agent Tool
Retrieve information about a specific tool available to the agent.
```javascript
const tool = await agent.getTool("tool-id");
```

### 7. Get Agent Evaluations
Obtain evaluation results for the agent.
- **Get CI Evaluations:**
```javascript
const evals = await agent.evals();
```
- **Get Live Evaluations:**
```javascript
const liveEvals = await agent.liveEvals();
```

**Last updated on February 20, 2025.**

----------------------------------------
Client Js/Error Handling
https://mastra.ai/docs/reference/client-js/error-handling
----------------------------------------

# Mastra Client SDK - Error Handling

## Overview
The Mastra Client SDK provides built-in error handling and a retry mechanism for API requests.

## Error Handling
All API methods can throw errors that should be caught and handled appropriately. 

### Example Code:
```javascript
try {
    const agent = client.getAgent("agent-id");
    const response = await agent.generate({
        messages: [{ role: "user", content: "Hello" }],
    });
} catch (error) {
    console.error("An error occurred:", error.message);
}
```

## Retry Mechanism
The SDK automatically retries failed requests using an exponential backoff strategy.

### Configuration Parameters:
- **baseUrl**: The base URL for the client (e.g., `"http://localhost:4111"`).
- **retries**: Number of retry attempts (e.g., `3`).
- **backoffMs**: Initial backoff time in milliseconds (e.g., `300`).
- **maxBackoffMs**: Maximum backoff time in milliseconds (e.g., `5000`).

### Example Code:
```javascript
const client = new MastraClient({
    baseUrl: "http://localhost:4111",
    retries: 3,         // Number of retry attempts
    backoffMs: 300,     // Initial backoff time
    maxBackoffMs: 5000, // Maximum backoff time
});
```

### Retry Process:
1. First attempt fails → Wait 300ms
2. Second attempt fails → Wait 600ms
3. Third attempt fails → Wait 1200ms
4. Final attempt fails → Throw error

_Last updated on February 20, 2025._

----------------------------------------
Client Js/Logs
https://mastra.ai/docs/reference/client-js/logs
----------------------------------------

# Logs API Documentation

The Logs API allows access to system logs and debugging information in Mastra.

## API Methods

### 1. Get Logs
Retrieve system logs with optional filtering.

**Method:**
```javascript
const logs = await client.getLogs({
  transportId: "transport-1"
});
```

**Parameters:**
- `transportId` (string): The identifier for the transport to filter logs.

---

### 2. Get Logs for a Specific Run
Retrieve logs for a specific execution run.

**Method:**
```javascript
const runLogs = await client.getLogForRun({
  runId: "run-1",
  transportId: "transport-1"
});
```

**Parameters:**
- `runId` (string): The identifier for the specific run.
- `transportId` (string): The identifier for the transport to filter logs.

---

**Last Updated:** February 20, 2025

----------------------------------------
Client Js/Memory
https://mastra.ai/docs/reference/client-js/memory
----------------------------------------

# Memory API Overview

The Memory API allows for managing conversation threads and message history in Mastra. Below are the key operations available through the API.

## Memory Thread Operations

### 1. Get All Threads
Retrieve all memory threads for a specific resource.

```javascript
const threads = await client.getMemoryThreads({
  resourceid: "resource-1",
});
```

### 2. Create a New Thread
Create a new memory thread.

```javascript
const thread = await client.createMemoryThread({
  title: "New Conversation",
  metadata: { category: "support" },
  resourceid: "resource-1",
});
```

### 3. Working with a Specific Thread
Get an instance of a specific memory thread.

```javascript
const thread = client.getMemoryThread("thread-id");
```

### Thread Methods

#### a. Get Thread Details
Retrieve details about a specific thread.

```javascript
const details = await thread.get();
```

#### b. Update Thread
Update properties of a thread.

```javascript
const updated = await thread.update({
  title: "Updated Title",
  metadata: { status: "resolved" },
  resourceid: "resource-1",
});
```

#### c. Delete Thread
Delete a thread and its messages.

```javascript
await thread.delete();
```

## Message Operations

### 1. Save Messages
Save messages to memory.

```javascript
const savedMessages = await client.saveMessageToMemory({
  messages: [
    {
      role: "user",
      content: "Hello!",
      id: "1",
      threadId: "thread-1",
      createdAt: new Date(),
      type: "text",
    },
  ],
});
```

### 2. Get Memory Status
Check the status of the memory system.

```javascript
const status = await client.getMemoryStatus();
```

_Last updated on February 20, 2025._

----------------------------------------
Client Js/Telemetry
https://mastra.ai/docs/reference/client-js/telemetry
----------------------------------------

# Telemetry API Documentation Summary

The Telemetry API allows you to retrieve and analyze traces from your Mastra application, aiding in monitoring and debugging application behavior and performance.

## Key Methods

### `getTelemetry()`

Retrieves traces with optional filtering and pagination.

#### Parameters:
- `name` (string, optional): Filter by trace name.
- `scope` (string, optional): Filter by scope.
- `page` (number, optional): Page number for pagination (default: 1).
- `perPage` (number, optional): Number of items per page (default: 10).
- `attribute` (object, optional): Filter by custom attributes.
  - `key` (string): The attribute key to filter by.

#### Example Code:
```javascript
const telemetry = await client.getTelemetry({
  name: "trace-name", // Optional: Filter by trace name
  scope: "scope-name", // Optional: Filter by scope
  page: 1, // Optional: Page number for pagination
  perPage: 10, // Optional: Number of items per page
  attribute: {
    key: "value", // Optional: Filter by custom attributes
  },
});
```

_Last updated on February 20, 2025_

----------------------------------------
Client Js/Tools
https://mastra.ai/docs/reference/client-js/tools
----------------------------------------

# Tools API Documentation Summary

The Tools API allows interaction with tools on the Mastra platform. Below are the key methods and their usage.

## API Methods

### 1. Get All Tools
Retrieve a list of all available tools.

```javascript
const tools = await client.getTools();
```

### 2. Get a Specific Tool
Get an instance of a specific tool using its ID.

```javascript
const tool = client.getTool("tool-id");
```

### 3. Get Tool Details
Retrieve detailed information about a specific tool.

```javascript
const details = await tool.details();
```

### 4. Execute Tool
Execute a tool with specific arguments. Optional parameters include `threadId` and `resourceid`.

```javascript
const result = await tool.execute({
  args: {
    param1: "value1",
    param2: "value2",
  },
  threadId: "thread-1", // Optional: Thread context
  resourceid: "resource-1", // Optional: Resource identifier
});
```

**Last updated on:** February 20, 2025

----------------------------------------
Client Js/Vectors
https://mastra.ai/docs/reference/client-js/vectors
----------------------------------------

# Vectors API Documentation Summary

The Vectors API allows interaction with vector embeddings for semantic search and similarity matching in Mastra. Below are the key methods and their details.

## Getting Started

To get an instance of a vector store:

```javascript
const vector = client.getVector("vector-name");
```

## Vector Methods

### 1. Get Vector Index Details

Retrieve information about a specific vector index:

```javascript
const details = await vector.details("index-name");
```

### 2. Create Vector Index

Create a new vector index:

```javascript
const result = await vector.createIndex({
  indexName: "new-index",
  dimension: 128,
  metric: "cosine" // Options: 'cosine', 'euclidean', 'dotproduct'
});
```

### 3. Upsert Vectors

Add or update vectors in an index:

```javascript
const ids = await vector.upsert({
  indexName: "my-index",
  vectors: [
    [0.1, 0.2, 0.3], // First vector
    [0.4, 0.5, 0.6]  // Second vector
  ],
  metadata: [{ label: "first" }, { label: "second" }],
  ids: ["id1", "id2"] // Optional: Custom IDs
});
```

### 4. Query Vectors

Search for similar vectors:

```javascript
const results = await vector.query({
  indexName: "my-index",
  queryVector: [0.1, 0.2, 0.3],
  topK: 10,
  filter: { label: "first" }, // Optional: Metadata filter
  includeVector: true // Optional: Include vectors in results
});
```

### 5. Get All Indexes

List all available indexes:

```javascript
const indexes = await vector.getIndexes();
```

### 6. Delete Index

Delete a vector index:

```javascript
const result = await vector.delete("index-name");
```

**Last updated on February 20, 2025.**

----------------------------------------
Client Js/Workflows
https://mastra.ai/docs/reference/client-js/workflows
----------------------------------------

# Workflows API Documentation Summary

The Workflows API allows interaction with automated workflows in Mastra. Below are the key methods and their usage.

## API Methods

### 1. Get All Workflows
Retrieve a list of all available workflows.

```javascript
const workflows = await client.getWorkflows();
```

### 2. Get a Specific Workflow
Get an instance of a specific workflow using its ID.

```javascript
const workflow = client.getWorkflow("workflow-id");
```

### 3. Get Workflow Details
Retrieve detailed information about a specific workflow.

```javascript
const details = await workflow.details();
```

### 4. Execute Workflow
Execute a workflow with input parameters.

```javascript
const result = await workflow.execute({
  input: {
    param1: "value1",
    param2: "value2",
  },
});
```

### 5. Resume Workflow
Resume a suspended workflow step using the step ID and run ID.

```javascript
const result = await workflow.resume({
  stepId: "step-id",
  runId: "run-id",
  contextData: { key: "value" },
});
```

### 6. Watch Workflow
Watch workflow transitions in real-time and process the transition data.

```javascript
const response = await workflow.watch();

// Read Transition Data from response body
const reader = response.body.getReader();
let buffer = "";

while (true) {
  const { done, value } = await reader.read();
  if (done) break;

  buffer += new TextDecoder().decode(value);
  const records = buffer.split("\x1E");
  buffer = records.pop() || "";

  for (const record of records) {
    const { activePaths, context, timestamp } = record;
    console.log({
      activePaths,
      context,
      timestamp,
    });
  }
}
```

## Last Updated
February 20, 2025

----------------------------------------
Core/Mastra Class
https://mastra.ai/docs/reference/core/mastra-class
----------------------------------------

### Mastra Class Documentation Summary

The **Mastra Class** is the primary entry point for applications using the Mastra framework. It is responsible for managing agents, workflows, and server endpoints.

#### Constructor Options
- **agents**: `Agent[]` (default: `[]`)
  - Array of Agent instances to register.
  
- **tools**: `Record<string, ToolApi>` (default: `{}`)
  - Custom tools to register as key-value pairs (tool name: tool function).
  
- **integrations**: `Integration[]` (default: `[]`)
  - Array of Mastra integrations to register for use by agents, workflows, and tools.
  
- **engine**: `MastraEngine`
  - Database engine instance.
  
- **vectors**: `Record<string, MastraVector>`
  - Vector store instance for semantic search and vector-based tools (e.g., Pinecone, PgVector, Qdrant).
  
- **logger**: `Logger` (default: `Console logger with INFO level`)
  - Logger instance created with `createLogger()`.
  
- **workflows**: `Record<string, Workflow>` (default: `{}`)
  - Workflows to register as key-value pairs (workflow name: workflow instance).

#### Initialization
Typically initialized in `src/mastra/index.ts`:

```javascript
import { Mastra } from "@mastra/core";
import { createLogger } from "@mastra/core/logger";

// Basic initialization
export const mastra = new Mastra({});

// Full initialization with all options
export const mastra = new Mastra({
  agents: [],
  workflows: {},
  integrations: [],
  logger: createLogger({
    name: "My Project",
    level: "info",
  }),
  engine: {},
  tools: {},
  vectors: {},
});
```

The Mastra class acts as a top-level registry, allowing registered tools to be utilized by agents and workflows, and integrations to be shared across all components.

#### Methods
- **getAgent(name: string): Agent**
  - Returns an agent instance by ID. Throws an error if the agent is not found.

- **setLogger({ key: string, logger: Logger }): void**
  - Sets a logger for a specific component (AGENT | WORKFLOW).

- **getLogger(key: string): Logger | undefined**
  - Retrieves the logger for a specific component.

#### Error Handling
Mastra methods throw typed errors that can be caught:

```javascript
try {
  const tool = mastra.getTool("nonexistentTool");
} catch (error) {
  if (error instanceof Error) {
    console.log(error.message); // "Tool with name nonexistentTool not found"
  }
}
```

**Last updated on February 20, 2025**.

----------------------------------------
Evals/Answer Relevancy
https://mastra.ai/docs/reference/evals/answer-relevancy
----------------------------------------

# AnswerRelevancyMetric Documentation Summary

## Overview
The `AnswerRelevancyMetric` class evaluates the relevance of an LLM's output in response to an input query using a judge-based scoring system. It provides detailed scoring and reasoning based on the alignment of the answer with the query.

## Basic Usage

### Importing Required Modules
```javascript
import { openai } from "@ai-sdk/openai";
import { AnswerRelevancyMetric } from "@mastra/evals/llm";
```

### Configuring the Model for Evaluation
```javascript
const model = openai("gpt-4o-mini");

const metric = new AnswerRelevancyMetric(model, {
  uncertaintyWeight: 0.3, // Weight for 'unsure' verdicts
  scale: 1,                // Maximum score value
});
```

### Measuring Relevancy
```javascript
const result = await metric.measure(
  "What is the capital of France?",
  "Paris is the capital of France."
);

console.log(result.score); // Score from 0-1
console.log(result.info.reason); // Explanation of the score
```

## Constructor Parameters
- **model**: `LanguageModel` - Configuration for the model used to evaluate relevancy.
- **options**: `AnswerRelevancyMetricOptions` (optional) - Configuration options for the metric.
  - **uncertaintyWeight**: `number` (default: 0.3) - Weight for 'unsure' verdicts in scoring (0-1).
  - **scale**: `number` (default: 1) - Maximum score value.

## measure() Parameters
- **input**: `string` - The original query or prompt.
- **output**: `string` - The LLM's response to evaluate.

### Returns
- **score**: `number` - Relevancy score (0 to scale, default 0-1).
- **info**: `object` - Contains the reason for the score.
  - **reason**: `string` - Explanation of the score.

## Scoring Details
The metric evaluates relevancy based on:
- **Statement Analysis**: Breaks output into statements while preserving context.
- **Relevance Evaluation**:
  - **"yes"**: Full weight for direct matches.
  - **"unsure"**: Partial weight (default: 0.3) for approximate matches.
  - **"no"**: Zero weight for irrelevant content.

### Final Score Calculation
\[ \text{Final Score} = \left( \frac{\text{direct} + (\text{uncertainty} \times \text{partial})}{\text{total statements}} \right) \times \text{scale} \]

### Score Interpretation
- **1.0**: Perfect relevance.
- **0.7-0.9**: High relevance.
- **0.4-0.6**: Moderate relevance.
- **0.1-0.3**: Low relevance.
- **0.0**: No relevance.

## Example with Custom Configuration
```javascript
const metric = new AnswerRelevancyMetric(model, {
  uncertaintyWeight: 0.5, // Higher weight for uncertain verdicts
  scale: 5,                // Use 0-5 scale instead of 0-1
});

const result = await metric.measure(
  "What are the benefits of exercise?",
  "Regular exercise improves cardiovascular health, builds strength, and boosts mental wellbeing."
);

// Example output:
// {
//   score: 4.5,
//   info: {
//     reason: "The score is 4.5 out of 5 because the response directly addresses the query with specific, accurate benefits of exercise."
//   }
// }
```

## Related Metrics
- Prompt Alignment Metric
- Context Precision Metric
- Faithfulness Metric

_Last updated on February 20, 2025._

----------------------------------------
Evals/Bias
https://mastra.ai/docs/reference/evals/bias
----------------------------------------

# BiasMetric Class Documentation

## Overview
The `BiasMetric` class evaluates biases in LLM outputs, including gender, political, racial/ethnic, and geographical biases. It is essential for identifying unintended biases after model fine-tuning or optimizations.

## Basic Usage

### Imports
```javascript
import { openai } from "@ai-sdk/openai";
import { BiasMetric } from "@mastra/evals/llm";
```

### Configuration
```javascript
// Configure the model for evaluation
const model = openai("gpt-4o-mini");

const metric = new BiasMetric(model, {
  scale: 1 // Optional, default is 1
});
```

### Measuring Bias
```javascript
const result = await metric.measure(
  "Describe leadership qualities in the workplace",
  "Strong leaders are typically men who can make tough decisions..."
);

console.log(result.score); // Score from 0-1, higher means more biased
```

## Constructor Parameters
- **model**: `LanguageModel` - Configuration for the model used to evaluate bias.
- **options**: `BiasMetricOptions` (optional) - Configuration options for the metric.
  - **scale**: `number` - Maximum score value (default is 1).

## measure() Method Parameters
- **input**: `string` - The original query or prompt.
- **output**: `string` - The LLM's response to evaluate.

### Returns
- **score**: `number` - Bias score (0 to scale, default 0-1). Higher scores indicate more bias.
- **info**: `object` - Contains the reason for the score.
  - **reason**: `string` - Explanation of the score.

## Bias Categories
The metric evaluates several types of bias:
- **Gender Bias**: Discrimination or stereotypes based on gender.
- **Political Bias**: Prejudice against political ideologies or beliefs.
- **Racial/Ethnic Bias**: Discrimination based on race, ethnicity, or national origin.
- **Geographical Bias**: Prejudice based on location or regional stereotypes.

## Scoring Details
The metric analyzes bias through:
- Opinion identification and extraction.
- Presence of discriminatory language.
- Use of stereotypes or generalizations.
- Balance in perspective presentation.
- Loaded or prejudicial terminology.

### Scoring Process
1. Extracts opinions from text (identifies subjective statements, excludes factual claims).
2. Evaluates each opinion (checks for discriminatory language, assesses stereotypes, analyzes perspective balance).
3. Final score calculation: 
   \[
   \text{Final Score} = \left(\frac{\text{biased\_opinions}}{\text{total\_opinions}}\right) \times \text{scale}
   \]

### Score Interpretation
- **1.0**: Complete bias - all opinions contain bias.
- **0.7-0.9**: Significant bias - majority of opinions show bias.
- **0.4-0.6**: Moderate bias - mix of biased and neutral opinions.
- **0.1-0.3**: Minimal bias - most opinions show balanced perspectives.
- **0.0**: No detectable bias - opinions are balanced and neutral.

## Example with Different Types of Bias
```javascript
const results = await Promise.all([
  metric.measure(
    "Describe voting patterns",
    "These radical right-wing voters consistently vote against their interests..."
  ),
  metric.measure(
    "Describe workplace dynamics",
    "Modern offices have diverse teams working together based on merit..."
  )
]);

// Example outputs:
// Political bias example: { score: 1.0 }
// Unbiased example: { score: 0.0 }
```

## Related Metrics
- Toxicity Metric
- Faithfulness Metric
- Hallucination Metric
- Context Relevancy Metric

_Last updated on February 20, 2025._

----------------------------------------
Evals/Completeness
https://mastra.ai/docs/reference/evals/completeness
----------------------------------------

# CompletenessMetric Class Overview

The `CompletenessMetric` class evaluates the thoroughness of an LLM's output in covering key elements from the input text. It analyzes various linguistic components such as nouns, verbs, topics, and terms to generate a completeness score.

## Basic Usage

```javascript
import { CompletenessMetric } from "@mastra/evals/nlp";

const metric = new CompletenessMetric();

const result = await metric.measure(
  "Explain how photosynthesis works in plants using sunlight, water, and carbon dioxide.",
  "Plants use sunlight to convert water and carbon dioxide into glucose through photosynthesis."
);

console.log(result.score); // Coverage score from 0-1
console.log(result.info);  // Detailed metrics about element coverage
```

## `measure()` Method

### Parameters

- **input**: `string`  
  The original text containing key elements to be covered.

- **output**: `string`  
  The LLM's response to evaluate for completeness.

### Returns

- **score**: `number`  
  Completeness score (0-1) indicating the proportion of input elements covered in the output.

- **info**: `object`  
  Detailed metrics about element coverage:
  - **inputElements**: `string[]` - Key elements extracted from the input.
  - **outputElements**: `string[]` - Key elements found in the output.
  - **missingElements**: `string[]` - Input elements not found in the output.
  - **elementCounts**: `object` - Count of elements in input and output.

## Element Extraction Details

The metric extracts and analyzes the following types of elements:

- **Nouns**: Key objects, concepts, and entities.
- **Verbs**: Actions and states (converted to infinitive form).
- **Topics**: Main subjects and themes.
- **Terms**: Significant individual words.

### Extraction Process

- Normalization of text (removing diacritics, converting to lowercase).
- Splitting camelCase words.
- Handling word boundaries.
- Special handling of short words (3 characters or less).
- Deduplication of elements.

## Scoring Details

### Scoring Process

1. Extracts key elements:
   - Nouns and named entities.
   - Action verbs.
   - Topic-specific terms.
   - Normalized word forms.

2. Calculates coverage of input elements:
   - Exact matches for short terms (≤3 chars).
   - Substantial overlap (>60%) for longer terms.

3. Final score calculation:
   ```plaintext
   Final Score = (covered_elements / total_input_elements) * scale
   ```

### Score Interpretation

- **1.0**: Complete coverage - all input elements present.
- **0.7 - 0.9**: High coverage - most key elements included.
- **0.4 - 0.6**: Partial coverage - some key elements present.
- **0.1 - 0.3**: Low coverage - most key elements missing.
- **0.0**: No coverage - output lacks all input elements.

## Example with Analysis

```javascript
import { CompletenessMetric } from "@mastra/evals/nlp";

const metric = new CompletenessMetric();

const result = await metric.measure(
  "The quick brown fox jumps over the lazy dog",
  "A brown fox jumped over a dog"
);

// Example output:
// {
//   score: 0.75,
//   info: {
//     inputElements: ["quick", "brown", "fox", "jump", "lazy", "dog"],
//     outputElements: ["brown", "fox", "jump", "dog"],
//     missingElements: ["quick", "lazy"],
//     elementCounts: { input: 6, output: 4 }
//   }
// }
```

## Related Metrics

- Answer Relevancy Metric
- Content Similarity Metric
- Textual Difference Metric
- Keyword Coverage Metric

_Last updated on February 20, 2025_

----------------------------------------
Evals/Content Similarity
https://mastra.ai/docs/reference/evals/content-similarity
----------------------------------------

# ContentSimilarityMetric Class Documentation

## Overview
The `ContentSimilarityMetric` class measures the textual similarity between two strings, providing a score that indicates how closely they match. It allows for configurable options regarding case sensitivity and whitespace handling.

## Basic Usage

```javascript
import { ContentSimilarityMetric } from "@mastra/evals/nlp";

const metric = new ContentSimilarityMetric({
  ignoreCase: true,
  ignoreWhitespace: true
});

const result = await metric.measure("Hello, world!", "hello world");

console.log(result.score); // Similarity score from 0-1
console.log(result.info);  // Detailed similarity metrics
```

## Constructor Parameters

### `ContentSimilarityOptions`
- **ignoreCase**: `boolean` (default: `true`)
  - Whether to ignore case differences when comparing strings.
  
- **ignoreWhitespace**: `boolean` (default: `true`)
  - Whether to normalize whitespace when comparing strings.

## `measure()` Method

### Parameters
- **input**: `string`
  - The reference text to compare against.
  
- **output**: `string`
  - The text to evaluate for similarity.

### Returns
- **score**: `number`
  - Similarity score (0-1), where 1 indicates perfect similarity.
  
- **info**: `object`
  - Detailed similarity metrics.
  - **similarity**: `number`
    - Raw similarity score between the two texts.

## Scoring Details
The metric evaluates textual similarity through character-level matching and configurable text normalization. The scoring process includes:
1. Normalizing text:
   - Case normalization (if `ignoreCase: true`)
   - Whitespace normalization (if `ignoreWhitespace: true`)
   
2. Comparing processed strings using a string-similarity algorithm:
   - Analyzes character sequences
   - Aligns word boundaries
   - Considers relative positions
   - Accounts for length differences

### Score Interpretation
- **1.0**: Perfect match - identical texts
- **0.7-0.9**: High similarity - mostly matching content
- **0.4-0.6**: Moderate similarity - partial matches
- **0.1-0.3**: Low similarity - few matching patterns
- **0.0**: No similarity - completely different texts

## Example with Different Options

### Case-Sensitive Comparison
```javascript
const caseSensitiveMetric = new ContentSimilarityMetric({
  ignoreCase: false,
  ignoreWhitespace: true
});

const result1 = await caseSensitiveMetric.measure("Hello World", "hello world");
// Example output: { score: 0.75, info: { similarity: 0.75 } }
```

### Strict Whitespace Comparison
```javascript
const strictWhitespaceMetric = new ContentSimilarityMetric({
  ignoreCase: true,
  ignoreWhitespace: false
});

const result2 = await strictWhitespaceMetric.measure("Hello   World", "Hello World");
// Example output: { score: 0.85, info: { similarity: 0.85 } }
```

## Related Metrics
- Completeness Metric
- Textual Difference Metric
- Answer Relevancy Metric
- Keyword Coverage Metric

_Last updated on February 20, 2025_

----------------------------------------
Evals/Context Position
https://mastra.ai/docs/reference/evals/context-position
----------------------------------------

# ContextPositionMetric Documentation Summary

## Overview
The `ContextPositionMetric` class evaluates the ordering of context nodes based on their relevance to a given query and output. It utilizes position-weighted scoring to prioritize the placement of the most relevant context pieces earlier in the sequence.

## Basic Usage

### Importing Required Modules
```javascript
import { openai } from "@ai-sdk/openai";
import { ContextPositionMetric } from "@mastra/evals/llm";
```

### Configuring the Model
```javascript
const model = openai("gpt-4o-mini");
```

### Initializing ContextPositionMetric
```javascript
const metric = new ContextPositionMetric(model, {
  context: [
    "Photosynthesis is a biological process used by plants to create energy from sunlight.",
    "The process of photosynthesis produces oxygen as a byproduct.",
    "Plants need water and nutrients from the soil to grow.",
  ],
});
```

### Measuring Context Position
```javascript
const result = await metric.measure(
  "What is photosynthesis?",
  "Photosynthesis is the process by which plants convert sunlight into energy."
);

console.log(result.score); // Position score from 0-1
console.log(result.info.reason); // Explanation of the score
```

## Constructor Parameters
- **model**: `ModelConfig` - Configuration for the model used to evaluate context positioning.
- **options**: `ContextPositionMetricOptions` - Configuration options for the metric.

### ContextPositionMetricOptions
- **scale**: `number` (default = 1) - Maximum score value.
- **context**: `string[]` - Array of context pieces in their retrieval order.

## measure() Parameters
- **input**: `string` - The original query or prompt.
- **output**: `string` - The generated response to evaluate.

### Returns
- **score**: `number` - Position score (0 to scale, default 0-1).
- **info**: `object` - Contains the reason for the score.
  - **reason**: `string` - Detailed explanation of the score.

## Scoring Details
1. **Evaluates context relevance**:
   - Assigns binary relevance (yes/no) to each context piece.
   - Records position in sequence.
   - Documents relevance reasoning.

2. **Applies position weights**:
   - Earlier positions are weighted more heavily (weight = 1/(position + 1)).
   - Sums weights of relevant pieces.
   - Normalizes by maximum possible score.

3. **Final Score Calculation**:
   \[
   \text{Final Score} = \left(\frac{\text{weighted\_sum}}{\text{max\_possible\_sum}}\right) \times \text{scale}
   \]

### Score Interpretation
- **1.0**: Optimal - Most relevant context first.
- **0.7-0.9**: Good - Relevant context mostly early.
- **0.4-0.6**: Mixed - Relevant context scattered.
- **0.1-0.3**: Suboptimal - Relevant context mostly later.
- **0.0**: Poor - Relevant context at end or missing.

## Example with Analysis
```javascript
const metric = new ContextPositionMetric(model, {
  context: [
    "A balanced diet is important for health.",
    "Exercise strengthens the heart and improves blood circulation.",
    "Regular physical activity reduces stress and anxiety.",
    "Exercise equipment can be expensive.",
  ],
});

const result = await metric.measure(
  "What are the benefits of exercise?",
  "Regular exercise improves cardiovascular health and mental wellbeing."
);

// Example output:
// {
//   score: 0.5,
//   info: {
//     reason: "The score is 0.5 because while the second and third contexts are highly relevant to the benefits of exercise, they are not optimally positioned at the beginning of the sequence."
//   }
// }
```

## Related Metrics
- Context Precision Metric
- Answer Relevancy Metric
- Completeness Metric
- Context Relevancy Metric

_Last updated on February 20, 2025_

----------------------------------------
Evals/Context Precision
https://mastra.ai/docs/reference/evals/context-precision
----------------------------------------

# ContextPrecisionMetric Documentation Summary

## Overview
The `ContextPrecisionMetric` class evaluates the relevance and precision of context nodes used for generating expected outputs. It utilizes a judge-based system to analyze each context piece's contribution and provides a weighted score based on their position.

## Basic Usage

### Importing Required Modules
```javascript
import { openai } from "@ai-sdk/openai";
import { ContextPrecisionMetric } from "@mastra/evals/llm";
```

### Configuring the Model
```javascript
const model = openai("gpt-4o-mini");
```

### Initializing ContextPrecisionMetric
```javascript
const metric = new ContextPrecisionMetric(model, {
  context: [
    "Photosynthesis is a biological process used by plants to create energy from sunlight.",
    "Plants need water and nutrients from the soil to grow.",
    "The process of photosynthesis produces oxygen as a byproduct.",
  ],
});
```

### Measuring Context Precision
```javascript
const result = await metric.measure(
  "What is photosynthesis?",
  "Photosynthesis is the process by which plants convert sunlight into energy."
);

console.log(result.score); // Precision score from 0-1
console.log(result.info.reason); // Explanation of the score
```

## Constructor Parameters

### ContextPrecisionMetric(model, options)
- **model**: `LanguageModel` - Configuration for the model used to evaluate context relevance.
- **options**: `ContextPrecisionMetricOptions` - Configuration options for the metric.

#### ContextPrecisionMetricOptions
- **scale**: `number` (default: 1) - Maximum score value.
- **context**: `string[]` - Array of context pieces in their retrieval order.

## measure() Method Parameters

### measure(input, output)
- **input**: `string` - The original query or prompt.
- **output**: `string` - The generated response to evaluate.

### Returns
- **score**: `number` - Precision score (0 to scale, default 0-1).
- **info**: `object` - Contains the reason for the score.
  - **reason**: `string` - Detailed explanation of the score.

## Scoring Details
1. **Binary Relevance Assessment**:
   - Relevant context: 1
   - Irrelevant context: 0

2. **Mean Average Precision (MAP) Scoring**:
   - Computes precision at each position.
   - Weights earlier positions more heavily.
   - Normalizes to configured scale.

3. **Final Score Calculation**:
   - Final score = Mean Average Precision * scale.

### Score Interpretation
- **1.0**: All relevant context in optimal order.
- **0.7-0.9**: Mostly relevant context with good ordering.
- **0.4-0.6**: Mixed relevance or suboptimal ordering.
- **0.1-0.3**: Limited relevance or poor ordering.
- **0.0**: No relevant context.

## Example with Analysis
```javascript
const model = openai("gpt-4o-mini");
const metric = new ContextPrecisionMetric(model, {
  context: [
    "Exercise strengthens the heart and improves blood circulation.",
    "A balanced diet is important for health.",
    "Regular physical activity reduces stress and anxiety.",
    "Exercise equipment can be expensive.",
  ],
});

const result = await metric.measure(
  "What are the benefits of exercise?",
  "Regular exercise improves cardiovascular health and mental wellbeing."
);

// Example output:
// {
//   score: 0.75,
//   info: {
//     reason: "The score is 0.75 because the first and third contexts are highly relevant to the benefits mentioned in the output, while the second and fourth contexts are not directly related to exercise benefits. The relevant contexts are well-positioned at the beginning and middle of the sequence."
//   }
// }
```

## Related Metrics
- Answer Relevancy Metric
- Context Position Metric
- Completeness Metric
- Context Relevancy Metric

**Last updated on February 20, 2025**

----------------------------------------
Evals/Context Relevancy
https://mastra.ai/docs/reference/evals/context-relevancy
----------------------------------------

### ContextRelevancyMetric Class Overview

The `ContextRelevancyMetric` class evaluates the relevance of retrieved context in a Retrieval-Augmented Generation (RAG) pipeline. It utilizes an LLM-based evaluation system to assess how well the context matches the input query.

#### Basic Usage

```javascript
import { openai } from "@ai-sdk/openai";
import { ContextRelevancyMetric } from "@mastra/evals/llm";

// Configure the model for evaluation
const model = openai("gpt-4o-mini");

const metric = new ContextRelevancyMetric(model, {
  context: [
    "All data is encrypted at rest and in transit",
    "Two-factor authentication is mandatory",
    "The platform supports multiple languages",
    "Our offices are located in San Francisco"
  ]
});

const result = await metric.measure(
  "What are our product's security features?",
  "Our product uses encryption and requires 2FA."
);

console.log(result.score); // Score from 0-1
console.log(result.info.reason); // Explanation of the relevancy assessment
```

#### Constructor Parameters

- **model**: `LanguageModel`  
  Configuration for the model used to evaluate context relevancy.

- **options**: `ContextRelevancyMetricOptions`  
  Configuration options for the metric.

##### ContextRelevancyMetricOptions

- **scale**: `number` (default = 1)  
  Maximum score value.

- **context**: `string[]`  
  Array of retrieved context documents.

#### measure() Method Parameters

- **input**: `string`  
  The original query or prompt.

- **output**: `string`  
  The LLM's response to evaluate.

#### Returns

- **score**: `number`  
  Context relevancy score (0 to scale, default 0-1).

- **info**: `object`  
  Contains the reason for the score.
  - **reason**: `string`  
    Detailed explanation of the relevancy assessment.

#### Scoring Details

The metric evaluates context relevance through binary classification:
1. **Extracts statements from context**: Breaks down context into meaningful units while preserving semantic relationships.
2. **Evaluates statement relevance**: Assesses each statement against the query, counts relevant statements, and calculates the relevance ratio.
3. **Final score**: 
   \[
   \text{score} = \left(\frac{\text{relevant statements}}{\text{total statements}}\right) \times \text{scale}
   \]

#### Score Interpretation

- **1.0**: Perfect relevancy (all context is relevant).
- **0.7-0.9**: High relevancy (most context is relevant).
- **0.4-0.6**: Moderate relevancy (mix of relevant and irrelevant).
- **0.1-0.3**: Low relevancy (mostly irrelevant).
- **0.0**: No relevancy (completely irrelevant).

#### Example with Custom Configuration

```javascript
import { openai } from "@ai-sdk/openai";
import { ContextRelevancyMetric } from "@mastra/evals/llm";

// Configure the model for evaluation
const model = openai("gpt-4o-mini");

const metric = new ContextRelevancyMetric(model, {
  scale: 100, // Use 0-100 scale instead of 0-1
  context: [
    "Basic plan costs $10/month",
    "Pro plan includes advanced features at $30/month",
    "Enterprise plan has custom pricing",
    "Our company was founded in 2020",
    "We have offices worldwide"
  ]
});

const result = await metric.measure(
  "What are our pricing plans?",
  "We offer Basic, Pro, and Enterprise plans."
);

// Example output:
// {
//   score: 60,
//   info: {
//     reason: "3 out of 5 statements are relevant to pricing plans. The statements about company founding and office locations are not relevant."
//   }
// }
```

### Related Metrics

- Contextual Recall Metric
- Context Precision Metric
- Context Position Metric

_Last updated on February 20, 2025_

----------------------------------------
Evals/Contextual Recall
https://mastra.ai/docs/reference/evals/contextual-recall
----------------------------------------

### ContextualRecallMetric Class Overview

The `ContextualRecallMetric` class evaluates how well a language model (LLM) incorporates relevant information from provided context into its responses. It focuses on the completeness of information recall rather than precision.

#### Basic Usage

```javascript
import { openai } from "@ai-sdk/openai";
import { ContextualRecallMetric } from "@mastra/evals/llm";

// Configure the model for evaluation
const model = openai("gpt-4o-mini");

const metric = new ContextualRecallMetric(model, {
  context: [
    "Product features: cloud synchronization capability",
    "Offline mode available for all users",
    "Supports multiple devices simultaneously",
    "End-to-end encryption for all data"
  ]
});

const result = await metric.measure(
  "What are the key features of the product?",
  "The product includes cloud sync, offline mode, and multi-device support."
);

console.log(result.score); // Score from 0-1
```

#### Constructor Parameters

- **model**: `LanguageModel` - Configuration for the model used to evaluate contextual recall.
- **options**: `ContextualRecallMetricOptions` - Configuration options for the metric.

##### ContextualRecallMetricOptions

- **scale**: `number` (default: 1) - Maximum score value.
- **context**: `string[]` - Array of reference documents or information to check against.

#### measure() Method Parameters

- **input**: `string` - The original query or prompt.
- **output**: `string` - The LLM's response to evaluate.

#### Returns

- **score**: `number` - Recall score (0 to scale, default 0-1).
- **info**: `object` - Contains the reason for the score.
  - **reason**: `string` - Detailed explanation of the score.

#### Scoring Details

The metric evaluates recall by comparing the response content against relevant context items:

1. **Evaluates information recall**:
   - Identifies relevant items in context.
   - Tracks correctly recalled information.
   - Measures completeness of recall.

2. **Calculates recall score**:
   - Counts correctly recalled items.
   - Compares against total relevant items.
   - Computes coverage ratio.

3. **Final Score Calculation**:
   \[
   \text{Final Score} = \left(\frac{\text{correctly recalled items}}{\text{total relevant items}}\right) \times \text{scale}
   \]

#### Score Interpretation

- **1.0**: Perfect recall - all relevant information included.
- **0.7-0.9**: High recall - most relevant information included.
- **0.4-0.6**: Moderate recall - some relevant information missed.
- **0.1-0.3**: Low recall - significant information missed.
- **0.0**: No recall - no relevant information included.

#### Example with Custom Configuration

```javascript
import { openai } from "@ai-sdk/openai";
import { ContextualRecallMetric } from "@mastra/evals/llm";

// Configure the model for evaluation
const model = openai("gpt-4o-mini");

const metric = new ContextualRecallMetric(model, {
  scale: 100, // Use 0-100 scale instead of 0-1
  context: [
    "All data is encrypted at rest and in transit",
    "Two-factor authentication (2FA) is mandatory",
    "Regular security audits are performed",
    "Incident response team available 24/7"
  ]
});

const result = await metric.measure(
  "Summarize the company's security measures",
  "The company implements encryption for data protection and requires 2FA for all users."
);

// Example output:
// {
//   score: 50, // Only half of the security measures were mentioned
//   info: {
//     reason: "The score is 50 because only half of the security measures were mentioned in the response. The response missed the regular security audits and incident response team information."
//   }
// }
```

### Related Metrics

- Context Relevancy Metric
- Completeness Metric
- Summarization Metric

_Last updated on February 20, 2025_

----------------------------------------
Evals/Faithfulness
https://mastra.ai/docs/reference/evals/faithfulness
----------------------------------------

# FaithfulnessMetric Reference

The **FaithfulnessMetric** in Mastra evaluates the factual accuracy of an LLM's output against a provided context. It extracts claims from the output and verifies them, making it crucial for assessing the reliability of RAG pipeline responses.

## Basic Usage

### Importing Required Modules

```javascript
import { openai } from "@ai-sdk/openai";
import { FaithfulnessMetric } from "@mastra/evals/llm";
```

### Configuring the Model for Evaluation

```javascript
const model = openai("gpt-4o-mini");

const metric = new FaithfulnessMetric(model, {
  context: [
    "The company was established in 1995.",
    "Currently employs around 450-550 people.",
  ],
});
```

### Measuring Faithfulness

```javascript
const result = await metric.measure(
  "Tell me about the company.",
  "The company was founded in 1995 and has 500 employees."
);

console.log(result.score); // 1.0
console.log(result.info.reason); // "All claims are supported by the context."
```

## Constructor Parameters

- **model**: `LanguageModel` - Configuration for the model used to evaluate faithfulness.
- **options**: `FaithfulnessMetricOptions` - Additional options for configuring the metric.

### FaithfulnessMetricOptions

- **scale**: `number` (default: 1) - The maximum score value; the final score is normalized to this scale.
- **context**: `string[]` - Array of context chunks for claim verification.

## measure() Parameters

- **input**: `string` - The original query or prompt given to the LLM.
- **output**: `string` - The LLM's response to be evaluated for faithfulness.

### Returns

- **score**: `number` - A score between 0 and the configured scale, indicating the proportion of claims supported by the context.
- **info**: `object` - Contains the reason for the score.
  - **reason**: `string` - Explanation of the score, detailing supported, contradicted, or unverifiable claims.

## Scoring Details

### Evaluation Process

1. **Claim Analysis**:
   - Extracts claims (factual and speculative).
   - Verifies each claim against the context.
   - Assigns verdicts: "yes", "no", or "unsure".

2. **Score Calculation**:
   - Counts supported claims.
   - Divides by total claims.
   - Scales to the configured range.

### Final Score Calculation

\[ \text{Final Score} = \left( \frac{\text{supported\_claims}}{\text{total\_claims}} \right) \times \text{scale} \]

### Score Interpretation

- **1.0**: All claims supported.
- **0.7-0.9**: Most claims supported; few unverifiable.
- **0.4-0.6**: Mixed support; some contradictions.
- **0.1-0.3**: Limited support; many contradictions.
- **0.0**: No supported claims.

## Advanced Example

### Example with Mixed Claim Types

```javascript
const model = openai("gpt-4o-mini");

const metric = new FaithfulnessMetric(model, {
  context: [
    "The company had 100 employees in 2020.",
    "Current employee count is approximately 500.",
  ],
});

const result = await metric.measure(
  "What's the company's growth like?",
  "The company has grown from 100 employees in 2020 to 500 now, and might expand to 1000 by next year."
);

// Example output:
// {
//   score: 0.67,
//   info: {
//     reason: "The score is 0.67 because two claims are supported by the context (initial employee count of 100 in 2020 and current count of 500), while the future expansion claim is marked as unsure."
//   }
// }
```

## Related Metrics

- Answer Relevancy Metric
- Hallucination Metric
- Context Relevancy Metric

_Last updated on February 20, 2025_

----------------------------------------
Evals/Hallucination
https://mastra.ai/docs/reference/evals/hallucination
----------------------------------------

# HallucinationMetric Documentation

## Overview
The `HallucinationMetric` evaluates the factual accuracy of outputs generated by a Language Model (LLM) by comparing them against provided context. It identifies contradictions and measures hallucination levels.

## Basic Usage

### Imports
```javascript
import { openai } from "@ai-sdk/openai";
import { HallucinationMetric } from "@mastra/evals/llm";
```

### Configuration
Configure the model and the metric:
```javascript
const model = openai("gpt-4o-mini");

const metric = new HallucinationMetric(model, {
  context: [
    "Tesla was founded in 2003 by Martin Eberhard and Marc Tarpenning in San Carlos, California."
  ],
});
```

### Measure Method
Evaluate the output:
```javascript
const result = await metric.measure(
  "Tell me about Tesla's founding.",
  "Tesla was founded in 2004 by Elon Musk in California."
);

console.log(result.score); // Score from 0-1
console.log(result.info.reason); // Explanation of the score
```

### Example Output
```json
{
  "score": 0.67,
  "info": {
    "reason": "The score is 0.67 because two out of three statements from the context were contradicted by the output, while the location statement was not contradicted."
  }
}
```

## Constructor Parameters
- **model**: `LanguageModel` - Configuration for the model used to evaluate hallucination.
- **options**: `HallucinationMetricOptions` - Configuration options for the metric.

### HallucinationMetricOptions
- **scale**: `number` (default: 1) - Maximum score value.
- **context**: `string[]` - Array of context pieces used as the source of truth.

## measure() Parameters
- **input**: `string` - The original query or prompt.
- **output**: `string` - The LLM's response to evaluate.

### Returns
- **score**: `number` - Hallucination score (0 to scale, default 0-1).
- **info**: `object` - Contains the reason for the score.
  - **reason**: `string` - Explanation of the score and identified contradictions.

## Scoring Details
1. **Analyzes factual content**:
   - Extracts statements from context.
   - Identifies numerical values.
   - Maps statement relationships.
   
2. **Analyzes output for contradictions**:
   - Compares against context statements.
   - Marks direct conflicts as contradictions.
   - Evaluates numerical accuracy.

3. **Calculates hallucination score**:
   - Counts contradicted statements.
   - Divides by total statements.
   - Scales to configured range.

### Final Score Calculation
```plaintext
Final score = (contradicted_statements / total_statements) * scale
```

## Important Considerations
- Speculative language (e.g., “might”, “possibly”) does not count as contradictions.
- Additional information beyond context is allowed unless it conflicts directly.
- Empty outputs yield zero contradictions.
- Numerical evaluations consider scale-appropriate precision and contextual approximations.

### Score Interpretation
- **1.0**: Complete hallucination - contradicts all context statements.
- **0.75**: High hallucination - contradicts 75% of context statements.
- **0.5**: Moderate hallucination - contradicts half of context statements.
- **0.25**: Low hallucination - contradicts 25% of context statements.
- **0.0**: No hallucination - output aligns with all context statements.

## Example with Analysis
```javascript
const model = openai("gpt-4o-mini");

const metric = new HallucinationMetric(model, {
  context: [
    "OpenAI was founded in December 2015 by Sam Altman, Greg Brockman, and others.",
    "The company launched with a $1 billion investment commitment.",
    "Elon Musk was an early supporter but left the board in 2018."
  ],
});

const result = await metric.measure({
  input: "What are the key details about OpenAI?",
  output: "OpenAI was founded in 2015 by Elon Musk and Sam Altman with a $2 billion investment."
});

// Example output:
{
  "score": 0.33,
  "info": {
    "reason": "The score is 0.33 because one out of three statements from the context was contradicted (the investment amount was stated as $2 billion instead of $1 billion). The founding date was correct, and while the output's description of founders was incomplete, it wasn't strictly contradictory."
  }
}
```

## Related Metrics
- Faithfulness Metric
- Answer Relevancy Metric
- Context Precision Metric
- Context Relevancy Metric

_Last updated on February 20, 2025._

----------------------------------------
Evals/Keyword Coverage
https://mastra.ai/docs/reference/evals/keyword-coverage
----------------------------------------

### KeywordCoverageMetric Class

The `KeywordCoverageMetric` class evaluates how well an LLM’s output covers important keywords from the input text. It focuses on keyword presence while ignoring common words and stop words.

#### Basic Usage

```javascript
import { KeywordCoverageMetric } from "@mastra/evals/nlp";

const metric = new KeywordCoverageMetric();

const result = await metric.measure(
  "What are the key features of Python programming language?",
  "Python is a high-level programming language known for its simple syntax and extensive libraries."
);

console.log(result.score); // Coverage score from 0-1
console.log(result.info);  // Object containing detailed metrics about keyword coverage
```

#### `measure()` Method

**Parameters:**
- `input` (string): The original text containing keywords to be matched.
- `output` (string): The text to evaluate for keyword coverage.

**Returns:**
- `score` (number): Coverage score (0-1) representing the proportion of matched keywords.
- `info` (object): Detailed metrics about keyword coverage.
  - `matchedKeywords` (number): Number of keywords found in the output.
  - `totalKeywords` (number): Total number of keywords from the input.

#### Scoring Details

The metric evaluates keyword coverage by:
- Filtering out common words and stop words (e.g., “the”, “a”, “and”).
- Performing case-insensitive matching.
- Handling word form variations.
- Special handling of technical terms and compound words.

**Scoring Process:**
1. Processes keywords from input and output:
   - Filters out common words and stop words.
   - Normalizes case and word forms.
   - Handles special terms and compounds.
2. Calculates keyword coverage:
   - Matches keywords between texts.
   - Counts successful matches.
   - Computes coverage ratio.

**Final Score Calculation:**
\[ \text{Final Score} = \left(\frac{\text{matched\_keywords}}{\text{total\_keywords}}\right) \times \text{scale} \]

**Score Interpretation:**
- **1.0**: Perfect keyword coverage
- **0.7 - 0.9**: Good coverage with most keywords present
- **0.4 - 0.6**: Moderate coverage with some keywords missing
- **0.1 - 0.3**: Poor coverage with many keywords missing
- **0.0**: No keyword matches

#### Examples

1. **Perfect Coverage Example**
   ```javascript
   const result1 = await metric.measure(
     "The quick brown fox jumps over the lazy dog",
     "A quick brown fox jumped over a lazy dog"
   );
   // { score: 1.0, info: { matchedKeywords: 6, totalKeywords: 6 } }
   ```

2. **Partial Coverage Example**
   ```javascript
   const result2 = await metric.measure(
     "Python features include easy syntax, dynamic typing, and extensive libraries",
     "Python has simple syntax and many libraries"
   );
   // { score: 0.67, info: { matchedKeywords: 4, totalKeywords: 6 } }
   ```

3. **Technical Terms Example**
   ```javascript
   const result3 = await metric.measure(
     "Discuss React.js component lifecycle and state management",
     "React components have lifecycle methods and manage state"
   );
   // { score: 1.0, info: { matchedKeywords: 4, totalKeywords: 4 } }
   ```

#### Special Cases
- **Empty Input/Output**: Returns score of 1.0 if both are empty, 0.0 if only one is empty.
- **Single Word**: Treated as a single keyword.
- **Technical Terms**: Preserves compound technical terms (e.g., “React.js”, “machine learning”).
- **Case Differences**: “JavaScript” matches “javascript”.
- **Common Words**: Ignored in scoring to focus on meaningful keywords.

#### Related Metrics
- Completeness Metric
- Content Similarity Metric
- Answer Relevancy Metric
- Textual Difference Metric
- Context Relevancy Metric

_Last updated on February 20, 2025_

----------------------------------------
Evals/Prompt Alignment
https://mastra.ai/docs/reference/evals/prompt-alignment
----------------------------------------

# PromptAlignmentMetric Documentation Summary

## Overview
The `PromptAlignmentMetric` class evaluates how well an LLM's output adheres to specified prompt instructions using a judge-based system. It provides detailed reasoning for any deviations from the instructions.

## Basic Usage

### Importing Required Modules
```javascript
import { openai } from "@ai-sdk/openai";
import { PromptAlignmentMetric } from "@mastra/evals/llm";
```

### Configuration
1. **Model Setup**
   ```javascript
   const model = openai("gpt-4o-mini");
   ```

2. **Instructions Definition**
   ```javascript
   const instructions = [
       "Start sentences with capital letters",
       "End each sentence with a period",
       "Use present tense"
   ];
   ```

3. **Metric Initialization**
   ```javascript
   const metric = new PromptAlignmentMetric(model, {
       instructions,
       scale: 1
   });
   ```

4. **Measuring Output**
   ```javascript
   const result = await metric.measure(
       "describe the weather",
       "The sun is shining. Clouds float in the sky. A gentle breeze blows."
   );
   console.log(result.score); // Alignment score from 0-1
   console.log(result.info.reason); // Explanation of the score
   ```

## Constructor Parameters
- **model**: `LanguageModel` - Configuration for the model used to evaluate instruction alignment.
- **options**: `PromptAlignmentOptions` - Configuration options for the metric.

### PromptAlignmentOptions
- **instructions**: `string[]` - Array of instructions that the output should follow.
- **scale**: `number` (optional, default = 1) - Maximum score value.

## measure() Parameters
- **input**: `string` - The original prompt or query.
- **output**: `string` - The LLM's response to evaluate.

### Returns
- **score**: `number` - Alignment score (0 to scale, default 0-1).
- **info**: `object` - Contains detailed metrics about instruction compliance.
  - **reason**: `string` - Detailed explanation of the score and instruction compliance.

## Scoring Details
1. **Instruction Applicability**: Determines if each instruction applies to the context.
2. **Compliance Evaluation**: Assesses compliance for applicable instructions.
3. **Score Calculation**: 
   - Final score formula: 
   \[
   \text{Final Score} = \left(\frac{\text{followed instructions}}{\text{applicable instructions}}\right) \times \text{scale}
   \]

## Instruction Verdicts
- **“yes”**: Instruction is applicable and completely followed.
- **“no”**: Instruction is applicable but not followed or only partially followed.
- **“n/a”**: Instruction is not applicable to the given context.

## Important Considerations
- **Empty Outputs**: All formatting instructions are applicable and marked as “no”.
- **Domain-Specific Instructions**: Always applicable if related to the queried domain.
- **"n/a" Verdicts**: Used only for completely different domains and do not affect the final score.

## Score Interpretation
- **1.0**: All applicable instructions followed perfectly.
- **0.7-0.9**: Most applicable instructions followed.
- **0.4-0.6**: Mixed compliance with applicable instructions.
- **0.1-0.3**: Limited compliance with applicable instructions.
- **0.0**: No applicable instructions followed.

## Example Code
### Example 1
```javascript
const result = await metric.measure(
    "List three fruits",
    "• Apple is red and sweet; • Banana is yellow and curved; • Orange is citrus and round."
);
// Output: { score: 1.0, info: { reason: "All instructions were followed." } }
```

### Example 2
```javascript
const result2 = await metric.measure(
    "List three fruits",
    "1. Apple 2. Banana 3. Orange and Grape"
);
// Output: { score: 0.33, info: { reason: "Numbered lists were used instead of bullet points." } }
```

## Related Metrics
- Answer Relevancy Metric
- Keyword Coverage Metric

_Last updated on February 20, 2025_

----------------------------------------
Evals/Summarization
https://mastra.ai/docs/reference/evals/summarization
----------------------------------------

# SummarizationMetric Documentation

## Overview
The `SummarizationMetric` evaluates the effectiveness of a language model's (LLM) summary by assessing two key aspects: **alignment** (factual correctness) and **coverage** (inclusion of key information). The final score is determined by the minimum of these two scores, ensuring that both qualities are essential for a good summary.

## Basic Usage

### Importing Required Modules
```javascript
import { openai } from "@ai-sdk/openai";
import { SummarizationMetric } from "@mastra/evals/llm";
```

### Configuring the Model
```javascript
const model = openai("gpt-4o-mini");
const metric = new SummarizationMetric(model);
```

### Measuring a Summary
```javascript
const result = await metric.measure(
  "The company was founded in 1995 by John Smith. It started with 10 employees and grew to 500 by 2020. The company is based in Seattle.",
  "Founded in 1995 by John Smith, the company grew from 10 to 500 employees by 2020."
);

console.log(result.score); // Score from 0-1
console.log(result.info);  // Detailed metrics about the summary
```

## Constructor Parameters

### SummarizationMetric
- **model**: `LanguageModel` - Configuration for the model used to evaluate summaries.
- **options**: `SummarizationMetricOptions` (optional) - Configuration options for the metric.
  - **scale**: `number` (default: 1) - Maximum score value.

## measure() Parameters
- **input**: `string` - The original text to be summarized.
- **output**: `string` - The generated summary to evaluate.

### Returns
- **score**: `number` - Summarization score (0 to scale, default 0-1).
- **info**: `object` - Detailed metrics about the summary.
  - **reason**: `string` - Explanation of the score, including alignment and coverage aspects.
  - **alignmentScore**: `number` - Alignment score (0 to 1).
  - **coverageScore**: `number` - Coverage score (0 to 1).

## Scoring Details

### Components
1. **Alignment Score**: Measures factual correctness by:
   - Extracting claims from the summary.
   - Verifying each claim against the original text.
   - Assigning verdicts: “yes”, “no”, or “unsure”.

2. **Coverage Score**: Measures inclusion of key information by:
   - Generating key questions from the original text.
   - Checking if the summary answers these questions.
   - Evaluating completeness.

### Scoring Process
- **Alignment Score Calculation**:
  - `supported_claims / total_claims`
  
- **Coverage Score Calculation**:
  - `answerable_questions / total_questions`

- **Final Score**:
  - `min(alignment_score, coverage_score) * scale`

## Score Interpretation
- **1.0**: Perfect summary - completely factual and covers all key information.
- **0.7-0.9**: Strong summary with minor omissions or slight inaccuracies.
- **0.4-0.6**: Moderate quality with significant gaps or inaccuracies.
- **0.1-0.3**: Poor summary with major omissions or factual errors.
- **0.0**: Invalid summary - completely inaccurate or missing critical information.

## Example with Analysis
```javascript
const result = await metric.measure(
  "The electric car company Tesla was founded in 2003 by Martin Eberhard and Marc Tarpenning. Elon Musk joined in 2004 as the largest investor and became CEO in 2008. The company's first car, the Roadster, was launched in 2008.",
  "Tesla, founded by Elon Musk in 2003, revolutionized the electric car industry starting with the Roadster in 2008."
);

// Example output:
console.log(result);
// {
//   score: 0.5,
//   info: {
//     reason: "The score is 0.5 because while the coverage is good (0.75) - mentioning the founding year, first car model, and launch date - the alignment score is lower (0.5) due to incorrectly attributing the company's founding to Elon Musk instead of Martin Eberhard and Marc Tarpenning.",
//     alignmentScore: 0.5,
//     coverageScore: 0.75,
//   }
// }
```

## Related Metrics
- Faithfulness Metric
- Completeness Metric
- Contextual Recall Metric
- Hallucination Metric

_Last updated on February 20, 2025_

----------------------------------------
Evals/Textual Difference
https://mastra.ai/docs/reference/evals/textual-difference
----------------------------------------

# TextualDifferenceMetric Class Documentation

## Overview
The `TextualDifferenceMetric` class measures textual differences between two strings using sequence matching. It provides a similarity score and detailed metrics about the changes needed to transform one text into another.

## Basic Usage
```javascript
import { TextualDifferenceMetric } from "@mastra/evals/nlp";

const metric = new TextualDifferenceMetric();

const result = await metric.measure(
  "The quick brown fox",
  "The fast brown fox"
);

console.log(result.score); // Similarity ratio from 0-1
console.log(result.info);  // Detailed change metrics
```

## `measure()` Method

### Parameters
- **input**: `string` - The original text to compare against.
- **output**: `string` - The text to evaluate for differences.

### Returns
- **score**: `number` - Similarity ratio (0-1), where 1 indicates identical texts.
- **info**: `object` - Detailed metrics about the differences:
  - **confidence**: `number` - Confidence score based on length difference (0-1).
  - **ratio**: `number` - Raw similarity ratio between the texts.
  - **changes**: `number` - Number of change operations (insertions, deletions, replacements).
  - **lengthDiff**: `number` - Normalized difference in length between input and output (0-1).

## Scoring Details
The metric calculates the following:
- **Similarity Ratio**: Based on sequence matching (0-1).
- **Changes**: Count of non-matching operations needed.
- **Length Difference**: Normalized difference in text lengths.
- **Confidence**: Inversely proportional to length difference.

### Scoring Process
1. Analyzes textual differences:
   - Performs sequence matching.
   - Counts change operations.
   - Measures length differences.
2. Calculates metrics:
   - Computes similarity ratio.
   - Determines confidence score.
   - Combines into a weighted score.

### Final Score Interpretation
- **1.0**: Identical texts - no differences.
- **0.7-0.9**: Minor differences - few changes needed.
- **0.4-0.6**: Moderate differences - significant changes.
- **0.1-0.3**: Major differences - extensive changes.
- **0.0**: Completely different texts.

## Example with Analysis
```javascript
import { TextualDifferenceMetric } from "@mastra/evals/nlp";

const metric = new TextualDifferenceMetric();

const result = await metric.measure(
  "Hello world! How are you?",
  "Hello there! How is it going?"
);

// Example output:
// {
//   score: 0.65,
//   info: {
//     confidence: 0.95,
//     ratio: 0.65,
//     changes: 2,
//     lengthDiff: 0.05
//   }
// }
```

## Related Metrics
- Content Similarity Metric
- Completeness Metric
- Keyword Coverage Metric

_Last updated on February 20, 2025_

----------------------------------------
Evals/Tone Consistency
https://mastra.ai/docs/reference/evals/tone-consistency
----------------------------------------

# ToneConsistencyMetric Documentation

## Overview
The `ToneConsistencyMetric` class evaluates the emotional tone and sentiment consistency of text. It operates in two modes:
1. **Tone Consistency**: Compares tone between input/output pairs.
2. **Tone Stability**: Analyzes tone stability within a single text.

## Basic Usage

### Importing the Class
```javascript
import { ToneConsistencyMetric } from "@mastra/evals/nlp";
```

### Creating an Instance
```javascript
const metric = new ToneConsistencyMetric();
```

### Comparing Tone Between Input and Output
```javascript
const result1 = await metric.measure(
  "I love this amazing product!",
  "This product is wonderful and fantastic!"
);
```

### Analyzing Tone Stability in a Single Text
```javascript
const result2 = await metric.measure(
  "The service is excellent. The staff is friendly. The atmosphere is perfect.",
  "" // Empty string for single-text analysis
);
```

### Logging Results
```javascript
console.log(result1.score); // Tone consistency score (0-1)
console.log(result2.score); // Tone stability score (0-1)
```

## `measure()` Method Parameters
- **input**: `string` - The text to analyze for tone.
- **output**: `string` - Reference text for tone comparison (use empty string for stability analysis).

### Returns
- **score**: `number` - Tone consistency/stability score (0-1).
- **info**: `object` - Detailed tone information.

#### Info Object (Tone Comparison)
- **responseSentiment**: `number` - Sentiment score for the input text.
- **referenceSentiment**: `number` - Sentiment score for the output text.
- **difference**: `number` - Absolute difference between sentiment scores.

#### Info Object (Tone Stability)
- **avgSentiment**: `number` - Average sentiment score across sentences.
- **sentimentVariance**: `number` - Variance in sentiment between sentences.

## Scoring Details
The metric evaluates sentiment consistency through tone pattern analysis and mode-specific scoring.

### Scoring Process
1. **Analyzes tone patterns**:
   - Extracts sentiment features.
   - Computes sentiment scores.
   - Measures tone variations.

2. **Calculates mode-specific score**:
   - **Tone Consistency**:
     - Compares sentiment between texts.
     - Score = 1 - (sentiment_difference / max_difference).
   - **Tone Stability**:
     - Analyzes sentiment across sentences.
     - Score = 1 - (sentiment_variance / max_variance).

### Final Score
- Calculated as: `mode_specific_score * scale` (default scale is 0-1).

### Score Interpretation
- **1.0**: Perfect tone consistency/stability.
- **0.7-0.9**: Strong consistency with minor variations.
- **0.4-0.6**: Moderate consistency with noticeable shifts.
- **0.1-0.3**: Poor consistency with major tone changes.
- **0.0**: No consistency - completely different tones.

## Example with Both Modes

### Tone Consistency Mode
```javascript
const consistencyResult = await metric.measure(
  "This product is fantastic and amazing!",
  "The product is excellent and wonderful!"
);
// Example output:
// {
//   score: 0.95,
//   info: {
//     responseSentiment: 0.8,
//     referenceSentiment: 0.75,
//     difference: 0.05
//   }
// }
```

### Tone Stability Mode
```javascript
const stabilityResult = await metric.measure(
  "Great service! Friendly staff. Perfect atmosphere.",
  ""
);
// Example output:
// {
//   score: 0.9,
//   info: {
//     avgSentiment: 0.6,
//     sentimentVariance: 0.1
//   }
// }
```

## Related Metrics
- Content Similarity Metric
- Toxicity Metric

_Last updated on February 20, 2025._

----------------------------------------
Evals/Toxicity
https://mastra.ai/docs/reference/evals/toxicity
----------------------------------------

# ToxicityMetric Class Documentation

The **ToxicityMetric** class evaluates whether an LLM's output contains toxic elements such as racism, bias, or personal attacks. It employs a judge-based system to analyze responses for various forms of toxicity, including:

- Personal attacks
- Mockery or sarcasm
- Hate speech
- Dismissive statements
- Threats or intimidation

## Basic Usage

```javascript
import { openai } from "@ai-sdk/openai";
import { ToxicityMetric } from "@mastra/evals/llm";

// Configure the model for evaluation
const model = openai("gpt-4o-mini");

const metric = new ToxicityMetric(model, {
  scale: 1, // Default scale is 0-1
});

const result = await metric.measure(
  "How is Sarah as a person?",
  "Sarah is a dedicated team member who always delivers quality work."
);

console.log(result.score); // Score from 0-1 (0 = not toxic, 1 = toxic)
console.log(result.info.reason); // Explanation of the toxicity assessment
```

## Constructor Parameters

- **model**: `LanguageModel`  
  Configuration for the model used to evaluate toxicity.

- **options**: `ToxicityMetricOptions` (optional)  
  Configuration options for the metric.
  - **scale**: `number` (default is 1)  
    Maximum score value.

## measure() Method Parameters

- **input**: `string`  
  The original query or prompt.

- **output**: `string`  
  The LLM's response to evaluate.

## Returns

- **score**: `number`  
  Toxicity score (0 to scale, default 0-1).

- **info**: `object`  
  Detailed toxicity information.
  - **reason**: `string`  
    Explanation of the toxicity assessment.

## Scoring Details

The metric evaluates toxicity through multiple aspects:

1. Personal attacks
2. Mockery or sarcasm
3. Hate speech
4. Dismissive statements
5. Threats or intimidation

### Scoring Process

1. Analyzes toxic elements:
   - Identifies personal attacks and mockery
   - Detects hate speech and threats
   - Evaluates dismissive statements
   - Assesses severity levels

2. Calculates toxicity score:
   - Weighs detected elements
   - Combines severity ratings
   - Normalizes to scale

**Final Score Calculation**:  
`(toxicity_weighted_sum / max_toxicity) * scale`

### Score Interpretation

- **0.8-1.0**: Severe toxicity
- **0.4-0.7**: Moderate toxicity
- **0.1-0.3**: Mild toxicity
- **0.0**: No toxic elements detected

## Example with Custom Configuration

```javascript
import { openai } from "@ai-sdk/openai";

const model = openai("gpt-4o-mini");

const metric = new ToxicityMetric(model, {
  scale: 10, // Use 0-10 scale instead of 0-1
});

const result = await metric.measure(
  "What do you think about the new team member?",
  "The new team member shows promise but needs significant improvement in basic skills."
);
```

## Related Metrics

- Tone Consistency Metric
- Bias Metric

_Last updated on February 20, 2025._

----------------------------------------
Memory/Memory
https://mastra.ai/docs/reference/memory/Memory
----------------------------------------

# Memory Class Documentation Summary

The **Memory** class in Mastra is designed for managing conversation history and message storage. It supports persistent storage, semantic search, and efficient message retrieval, using **LibSQL** for storage and vector search, and **FastEmbed** for embeddings.

## Basic Usage

To use the Memory class, import it along with the Agent class:

```javascript
import { Memory } from "@mastra/memory";
import { Agent } from "@mastra/core/agent";

const agent = new Agent({
  memory: new Memory(),
  // ...otherOptions
});
```

## Custom Configuration

You can customize the Memory class with specific storage and vector database configurations:

```javascript
import { Memory } from "@mastra/memory";
import { DefaultStorage, DefaultVectorDB } from "@mastra/core/storage";
import { Agent } from "@mastra/core/agent";

const memory = new Memory({
  storage: new DefaultStorage({
    url: "file:memory.db",
  }),
  vector: new DefaultVectorDB({
    url: "file:vector.db",
  }),
  options: {
    lastMessages: 20,
    semanticRecall: {
      topK: 3,
      messageRange: {
        before: 2,
        after: 1,
      },
    },
    workingMemory: {
      enabled: true,
      template: "<user><first_name></first_name><last_name></last_name></user>",
    },
  },
});

const agent = new Agent({
  memory,
  // ...otherOptions
});
```

## Parameters

- **storage**: `MastraStorage` - Storage implementation for memory data.
- **vector**: `MastraVector` (optional) - Vector store for semantic search.
- **embedder**: `EmbeddingModel` (optional) - Embedder instance for vector embeddings (default: FastEmbed with bge-small-en-v1.5).
- **options**: `MemoryConfig` (optional) - General configuration options.

### Options

- **lastMessages**: `number | false` (default: 40) - Number of recent messages to retrieve.
- **semanticRecall**: `boolean | SemanticRecallConfig` (default: false) - Enables semantic search when a vector store is provided.
- **topK**: `number` (default: 2) - Number of similar messages to retrieve.
- **messageRange**: `number | { before: number; after: number }` (default: 2) - Range of messages around search results.
- **workingMemory**: `{ enabled: boolean; template: string }` (default: { enabled: false, template: '<user><first_name></first_name><last_name></last_name>...</user>' }) - Configuration for persistent user information storage.

## Working Memory

The working memory feature allows agents to maintain persistent information across conversations. It automatically manages XML-based updates through the conversation stream. If no template is provided, a default template is used.

## Embedder

By default, Memory uses **FastEmbed** with the **bge-small-en-v1.5** model. Specify a different embedder if needed.

## Additional Notes

### Vector Search Configuration

For custom vector search configurations, ensure both the vector store and search options are set:

```javascript
import { Memory } from "@mastra/memory";
import { DefaultStorage, DefaultVectorDB } from "@mastra/core/storage";

const memory = new Memory({
  storage: new DefaultStorage({
    url: ":memory:",
  }),
  vector: new DefaultVectorDB({
    url: ":memory:",
  }),
  options: {
    semanticRecall: {
      topK: 5,
      messageRange: 2,
    },
  },
});
```

### Related Methods

- **createThread**
- **query**

_Last updated on February 20, 2025._

----------------------------------------
Memory/Createthread
https://mastra.ai/docs/reference/memory/createThread
----------------------------------------

### Memory.createThread()

**Description:**  
Creates a new conversation thread in the memory system. Each thread represents a distinct conversation or context and can contain multiple messages.

**Usage Example:**
```javascript
import { Memory } from "@mastra/memory";

const memory = new Memory({ /* config */ });

const thread = await memory.createThread({
  resourceId: "user-123",
  title: "Support Conversation",
  metadata: {
    category: "support",
    priority: "high"
  }
});
```

**Parameters:**
- `resourceId` (string): Identifier for the resource this thread belongs to (e.g., user ID, project ID).
- `threadId?` (string): Optional custom ID for the thread. If not provided, one will be generated.
- `title?` (string): Optional title for the thread.
- `metadata?` (Record<string, unknown>): Optional metadata to associate with the thread.

**Returns:**
- `id` (string): Unique identifier of the created thread.
- `resourceId` (string): Resource ID associated with the thread.
- `title` (string): Title of the thread (if provided).
- `createdAt` (Date): Timestamp when the thread was created.
- `updatedAt` (Date): Timestamp when the thread was last updated.
- `metadata` (Record<string, unknown>): Additional metadata associated with the thread.

**Related Methods:**
- `Memory.getThreadById()`
- `Memory.getThreadsByResourceId()`

**Last Updated:** February 20, 2025

----------------------------------------
Memory/Getthreadbyid
https://mastra.ai/docs/reference/memory/getThreadById
----------------------------------------

### Memory API: `getThreadById()`

The `getThreadById` function retrieves a specific thread from storage using its ID.

#### Usage Example

```javascript
import { Memory } from "@mastra/core/memory";

const memory = new Memory(config);

const thread = await memory.getThreadById({ threadId: "thread-123" });
```

#### Parameters

- **threadId**: `string`  
  The ID of the thread to be retrieved.

#### Returns

- **Promise<StorageThreadType | null>**  
  Resolves to the thread associated with the given ID, or `null` if not found.

#### Related Methods

- `.query()`
- `.getThreadsByResourceId()`

_Last updated on February 20, 2025._

----------------------------------------
Memory/Getthreadsbyresourceid
https://mastra.ai/docs/reference/memory/getThreadsByResourceId
----------------------------------------

### API Method: `getThreadsByResourceId()`

**Description**:  
Retrieves all threads associated with a specific resource ID from storage.

**Usage Example**:
```javascript
import { Memory } from "@mastra/core/memory";

const memory = new Memory(config);

const threads = await memory.getThreadsByResourceId({
  resourceId: "resource-123",
});
```

**Parameters**:
- `resourceId` (string): The ID of the resource whose threads are to be retrieved.

**Returns**:
- `Promise<StorageThreadType[]>`: A promise that resolves to an array of threads associated with the given resource ID.

**Related Methods**: 
- `getThreadById()`
- `chunk()`

**Last Updated**: February 20, 2025

----------------------------------------
Memory/Query
https://mastra.ai/docs/reference/memory/query
----------------------------------------

### Memory API Reference

#### Method: `Memory.query()`

**Description:**  
Retrieves messages from a specific thread with pagination and filtering options.

**Usage Example:**

```javascript
import { Memory } from "@mastra/memory";

const memory = new Memory({
  /* config */
});

// Get last 50 messages
const { messages, uiMessages } = await memory.query({
  threadId: "thread-123",
  selectBy: {
    last: 50,
  },
});

// Get messages with context around specific messages
const { messages: contextMessages } = await memory.query({
  threadId: "thread-123",
  selectBy: {
    include: [
      {
        id: "msg-123", // Get just this message (no context)
      },
      {
        id: "msg-456", // Get this message with custom context
        withPreviousMessages: 3, // 3 messages before
        withNextMessages: 1, // 1 message after
      },
    ],
  },
});

// Semantic search in messages
const { messages } = await memory.query({
  threadId: "thread-123",
  selectBy: {
    vectorSearchString: "What was discussed about deployment?",
  },
  threadConfig: {
    historySearch: true,
  },
});
```

#### Parameters:

- **threadId**: `string`  
  Unique identifier of the thread to retrieve messages from.

- **selectBy**: `object` (optional)  
  Options for filtering messages.

  - **vectorSearchString**: `string` (optional)  
    Search string for finding semantically similar messages.

  - **last**: `number | false` (default: 40)  
    Number of most recent messages to retrieve. Set to false to disable limit.

  - **include**: `array` (optional)  
    Array of message IDs to include with context.

    - **id**: `string`  
      ID of the message to include.

    - **withPreviousMessages**: `number` (optional)  
      Number of messages to include before this message. Defaults to 2 when using vector search, 0 otherwise.

    - **withNextMessages**: `number` (optional)  
      Number of messages to include after this message. Defaults to 2 when using vector search, 0 otherwise.

- **threadConfig**: `MemoryConfig` (optional)  
  Configuration options for message retrieval.

#### Returns:

- **messages**: `CoreMessage[]`  
  Array of retrieved messages in their core format.

- **uiMessages**: `AiMessage[]`  
  Array of messages formatted for UI display.

#### Additional Notes:

- The `query` function returns two message formats:
  - **messages**: Core message format used internally.
  - **uiMessages**: Formatted messages suitable for UI display, including proper threading of tool calls and results.

#### Related Methods:
- `Memory.createThread()`
- `Memory.getThreadById()`

_Last updated on February 20, 2025._

----------------------------------------
Observability/Create Logger
https://mastra.ai/docs/reference/observability/create-logger
----------------------------------------

### API Reference: `createLogger()`

The `createLogger()` function instantiates a logger based on specified configurations. It supports console-based, file-based, and Upstash Redis-based loggers.

#### Usage Examples

1. **Console Logger (Development)**

   ```javascript
   const consoleLogger = createLogger({ 
       name: "Mastra", 
       level: "debug" 
   });

   consoleLogger.info("App started");
   ```

2. **File Transport (Structured Logs)**

   ```javascript
   import { FileTransport } from "@mastra/loggers/file";

   const fileLogger = createLogger({
       name: "Mastra",
       transports: { 
           file: new FileTransport({ path: "test-dir/test.log" }) 
       },
       level: "warn",
   });

   fileLogger.warn("Low disk space", {
       destinationPath: "system",
       type: "WORKFLOW",
   });
   ```

3. **Upstash Logger (Remote Log Drain)**

   ```javascript
   import { UpstashTransport } from "@mastra/loggers/upstash";

   const logger = createLogger({
       name: "Mastra",
       transports: {
           upstash: new UpstashTransport({
               listName: "production-logs",
               upstashUrl: process.env.UPSTASH_URL,
               upstashToken: process.env.UPSTASH_TOKEN,
           }),
       },
       level: "info",
   });

   logger.info({
       message: "User signed in",
       destinationPath: "auth",
       type: "AGENT",
       runId: "run_123",
   });
   ```

#### Parameters

- **type**: Specifies the logger implementation to create.
- **level** (optional): Minimum severity level of logs to record (DEBUG, INFO, WARN, ERROR).
- **dirPath** (optional): For FILE type only. Directory path for log files (default: "logs").
- **url** (optional): For UPSTASH type only. Upstash Redis endpoint URL for storing logs.
- **token** (optional): For UPSTASH type only. Upstash Redis access token.
- **key** (optional): For UPSTASH type only. Redis list key under which logs are stored.

_Last updated on February 20, 2025_

----------------------------------------
Observability/Logger
https://mastra.ai/docs/reference/observability/logger
----------------------------------------

# Logger Instance Documentation

## Overview
A Logger instance is created using the `createLogger()` function. It provides methods to log events at various severity levels, with output directed to the console, file, or an external service depending on the logger type.

## Example Usage

```javascript
// Using a console logger
const logger = createLogger({ name: 'Mastra', level: 'info' });

logger.debug('Debug message'); 
// Won't be logged because level is INFO

logger.info({ message: 'User action occurred', destinationPath: 'user-actions', type: 'AGENT' }); 
// Logged

logger.error('An error occurred'); 
// Logged as ERROR
```

## Logger Methods

### 1. `debug(message: BaseLogMessage | string, ...args: any[]) => void | Promise<void>`
- **Description**: Write a DEBUG-level log.
- **Condition**: Recorded only if level ≤ DEBUG.

### 2. `info(message: BaseLogMessage | string, ...args: any[]) => void | Promise<void>`
- **Description**: Write an INFO-level log.
- **Condition**: Recorded only if level ≤ INFO.

### 3. `warn(message: BaseLogMessage | string, ...args: any[]) => void | Promise<void>`
- **Description**: Write a WARN-level log.
- **Condition**: Recorded only if level ≤ WARN.

### 4. `error(message: BaseLogMessage | string, ...args: any[]) => void | Promise<void>`
- **Description**: Write an ERROR-level log.
- **Condition**: Recorded only if level ≤ ERROR.

### 5. `cleanup?() => Promise<void>`
- **Description**: Cleanup resources held by the logger (e.g., network connections for Upstash).
- **Note**: Not all loggers implement this method.

## Important Note
Some loggers require a `BaseLogMessage` object, which includes `message`, `destinationPath`, and `type` fields. For example, the File and Upstash loggers need structured messages.

_Last updated on February 20, 2025_

----------------------------------------
Observability/Otel Config
https://mastra.ai/docs/reference/observability/otel-config
----------------------------------------

### OtelConfig Overview

The `OtelConfig` object configures OpenTelemetry instrumentation, tracing, and exporting behavior in applications. Adjusting its properties allows control over telemetry data collection, sampling, and export.

### Usage in Mastra

To use `OtelConfig` with Mastra, pass it as the value of the `telemetry` key during initialization. This integrates custom OpenTelemetry settings for tracing and instrumentation.

### Example Code

```javascript
import { Mastra } from 'mastra';

const otelConfig: OtelConfig = {
  serviceName: 'my-awesome-service',
  enabled: true,
  sampling: {
    type: 'ratio',
    probability: 0.5,
  },
  export: {
    type: 'otlp',
    endpoint: 'https://otel-collector.example.com/v1/traces',
    headers: {
      Authorization: 'Bearer YOUR_TOKEN_HERE',
    },
  },
};

// Initialize Mastra with OtelConfig
Mastra.init({ telemetry: otelConfig });
```

### Properties

- **serviceName**: `string`  
  Human-readable name for identifying the service in telemetry backends.

- **enabled**: `boolean`  
  Indicates if telemetry collection and export are enabled.

- **sampling**: `SamplingStrategy`  
  Defines the sampling strategy for traces:
  - `ratio`
  - `always_on`
  - `always_off`
  - `parent_based`
  
  Probability: `number` (0.0 to 1.0)

- **export**: `object`  
  Configuration for exporting telemetry data:
  - **type**: `'otlp' | 'console'` (string)
  - **endpoint**: `string`
  - **headers**: `Record<string, string>`

### Last Updated
February 20, 2025

----------------------------------------
Observability/Providers
https://mastra.ai/docs/reference/observability/providers
----------------------------------------

# Observability Providers Overview

## List of Observability Providers
- **SigNoz**
- **Braintrust**
- **Langfuse**
- **Langsmith**
- **New Relic**
- **Traceloop**
- **Laminar**

**Last Updated:** February 20, 2025

## API Method: `.watch()`
- **Description:** This method is used to monitor observability metrics from the providers listed above. 

### Example Code
```javascript
// Example usage of the .watch() method
observabilityProvider.watch({
    provider: 'SigNoz',
    metrics: ['response_time', 'error_rate']
});
```

This summary provides a concise overview of observability providers and includes an example of how to use the `.watch()` method effectively.

----------------------------------------
Rag/Astra
https://mastra.ai/docs/reference/rag/astra
----------------------------------------

# AstraVector Class Documentation

## Overview
The `AstraVector` class enables vector search functionality using DataStax Astra DB, a cloud-native, serverless database based on Apache Cassandra. It offers enterprise-grade scalability and high availability for vector search operations.

## Constructor Options
- **token**: `string` - Astra DB API token.
- **endpoint**: `string` - Astra DB API endpoint.
- **keyspace**: `string?` - Optional keyspace name.

## Methods

### 1. `createIndex()`
- **Parameters**:
  - `indexName`: `string` - Name of the index to create.
  - `dimension`: `number` - Vector dimension (must match your embedding model).
  - `metric`: `'cosine' | 'euclidean' | 'dotproduct'` (default: `cosine`) - Distance metric for similarity search.

### 2. `upsert()`
- **Parameters**:
  - `indexName`: `string` - Name of the index to upsert into.
  - `vectors`: `number[][]` - Array of embedding vectors.
  - `metadata`: `Record<string, any>[]?` - Metadata for each vector.
  - `ids`: `string[]?` - Optional vector IDs (auto-generated if not provided).

### 3. `query()`
- **Parameters**:
  - `indexName`: `string` - Name of the index to query.
  - `queryVector`: `number[]` - Query vector to find similar vectors.
  - `topK`: `number?` (default: `10`) - Number of results to return.
  - `filter`: `Record<string, any>?` - Metadata filters for the query.
  - `includeVector`: `boolean?` (default: `false`) - Whether to include vectors in the results.

### 4. `listIndexes()`
- **Returns**: `string[]` - An array of index names.

### 5. `describeIndex()`
- **Parameters**:
  - `indexName`: `string` - Name of the index to describe.
- **Returns**: 
  - `IndexStats` interface:
    - `dimension`: `number`
    - `count`: `number`
    - `metric`: `'cosine' | 'euclidean' | 'dotproduct'`

### 6. `deleteIndex()`
- **Parameters**:
  - `indexName`: `string` - Name of the index to delete.

## Response Types
- **Query Results**: 
  - `QueryResult` interface:
    - `id`: `string`
    - `score`: `number`
    - `metadata`: `Record<string, any>`

## Error Handling
Use the following structure to handle errors:
```javascript
try {
  await store.query("index_name", queryVector);
} catch (error) {
  if (error instanceof VectorStoreError) {
    console.log(error.code); // e.g., 'connection_failed', 'invalid_dimension'
    console.log(error.details); // Additional error context
  }
}
```

## Environment Variables
- **ASTRA_DB_TOKEN**: Your Astra DB API token.
- **ASTRA_DB_ENDPOINT**: Your Astra DB API endpoint.

## Related Topics
- Metadata Filters

_Last updated on February 20, 2025_

----------------------------------------
Rag/Chroma
https://mastra.ai/docs/reference/rag/chroma
----------------------------------------

# ChromaVector Class Documentation Summary

The **ChromaVector** class enables vector search using **ChromaDB**, an open-source embedding database. It supports efficient vector search with metadata filtering and hybrid search capabilities.

## Constructor Options

- **path**: `string` - URL path to the ChromaDB instance.
- **auth**: `object` (optional) - Authentication configuration.
  - **provider**: `string` - Authentication provider.
  - **credentials**: `string` - Authentication credentials.

## Methods

### 1. `createIndex()`
- **Parameters**:
  - **indexName**: `string` - Name of the index to create.
  - **dimension**: `number` - Vector dimension (must match your embedding model).
  - **metric**: `'cosine' | 'euclidean' | 'dotproduct'` (default: `cosine`) - Distance metric for similarity search.

### 2. `upsert()`
- **Parameters**:
  - **indexName**: `string` - Name of the index to upsert into.
  - **vectors**: `number[][]` - Array of embedding vectors.
  - **metadata**: `Record<string, any>[]` (optional) - Metadata for each vector.
  - **ids**: `string[]` (optional) - Optional vector IDs (auto-generated if not provided).

### 3. `query()`
- **Parameters**:
  - **indexName**: `string` - Name of the index to query.
  - **queryVector**: `number[]` - Query vector to find similar vectors.
  - **topK**: `number` (default: `10`) - Number of results to return.
  - **filter**: `Record<string, any>` (optional) - Metadata filters for the query.
  - **includeVector**: `boolean` (default: `false`) - Whether to include vectors in the results.

### 4. `listIndexes()`
- **Returns**: An array of index names as strings.

### 5. `describeIndex()`
- **Parameters**:
  - **indexName**: `string` - Name of the index to describe.
- **Returns**: An interface `IndexStats` containing:
  - **dimension**: `number`
  - **count**: `number`
  - **metric**: `"cosine" | "euclidean" | "dotproduct"`

### 6. `deleteIndex()`
- **Parameters**:
  - **indexName**: `string` - Name of the index to delete.

## Response Types

Query results are returned in the following format:

```typescript
interface QueryResult {
  id: string;
  score: number;
  metadata: Record<string, any>;
}
```

## Error Handling

The store throws typed errors that can be caught as follows:

```typescript
try {
  await store.query("index_name", queryVector);
} catch (error) {
  if (error instanceof VectorStoreError) {
    console.log(error.code); // 'connection_failed' | 'invalid_dimension' | etc.
    console.log(error.details); // Additional error context.
  }
}
```

## Related

- Metadata Filters

_Last updated on February 20, 2025._

----------------------------------------
Rag/Chunk
https://mastra.ai/docs/reference/rag/chunk
----------------------------------------

### .chunk() Method Reference

The `.chunk()` function is used to split documents into smaller segments using various strategies and options.

#### Example Usage

```javascript
import { Document } from '@mastra/core';

const doc = new Document(`
# Introduction
This is a sample document that we want to split into chunks.

## Section 1
Here is the first section with some content.

## Section 2 
Here is another section with different content.
`);

// Basic chunking with defaults
const chunks = await doc.chunk();

// Markdown-specific chunking with header extraction
const chunksWithMetadata = await doc.chunk({
  strategy: 'markdown',
  headers: [['#', 'title'], ['##', 'section']],
  extract: {
    fields: [
      { name: 'summary', description: 'A brief summary of the chunk content' },
      { name: 'keywords', description: 'Key terms found in the chunk' }
    ]
  }
});
```

#### Parameters

- **strategy**: `string` (optional)
  - Options: `'recursive' | 'character' | 'token' | 'markdown' | 'html' | 'json' | 'latex'`
  - Default behavior based on document type.
  
- **size**: `number` (optional, default: `512`)
  - Maximum size of each chunk.
  
- **overlap**: `number` (optional, default: `50`)
  - Number of characters/tokens that overlap between chunks.
  
- **separator**: `string` (optional, default: `\n\n`)
  - Character(s) to split on.
  
- **isSeparatorRegex**: `boolean` (optional, default: `false`)
  - Whether the separator is a regex pattern.
  
- **keepSeparator**: `'start' | 'end'` (optional)
  - Whether to keep the separator at the start or end of chunks.
  
- **extract**: `ExtractParams` (optional)
  - Metadata extraction configuration.

#### Strategy-Specific Options

- **HTML Strategy**:
  ```javascript
  const chunks = await doc.chunk({
    strategy: 'html',
    headers: [['h1', 'title'], ['h2', 'subtitle']],
    sections: [['div.content', 'main']],
    size: 500
  });
  ```

- **Markdown Strategy**:
  ```javascript
  const chunks = await doc.chunk({
    strategy: 'markdown',
    headers: [['#', 'title'], ['##', 'section']],
    stripHeaders: true,
    overlap: 50
  });
  ```

- **Token Strategy**:
  ```javascript
  const chunks = await doc.chunk({
    strategy: 'token',
    encodingName: 'gpt2',
    modelName: 'gpt-3.5-turbo',
    size: 1000
  });
  ```

#### Return Value

Returns a `MDocument` instance containing the chunked documents. Each chunk includes:

```typescript
interface DocumentNode {
  text: string;
  metadata: Record<string, any>;
  embedding?: number[];
}
```

#### Last Updated
February 20, 2025

----------------------------------------
Rag/Default
https://mastra.ai/docs/reference/rag/default
----------------------------------------

# DefaultVectorDB Documentation Summary

## Overview
The `DefaultVectorDB` class provides an efficient vector search solution using LibSQL (a SQLite fork with vector extensions) and Turso. It is part of the `@mastra/core` package and supports vector similarity search with metadata filtering.

## Installation
To install the core package, use:
```bash
npm install @mastra/core
```

## Usage
### Importing the Class
```javascript
import { DefaultVectorDB } from '@mastra/core/storage';
```

### Creating a Vector Store Instance
```javascript
const store = new DefaultVectorDB({
  connectionUrl: process.env.DATABASE_URL, // LibSQL database URL
  authToken: process.env.DATABASE_AUTH_TOKEN // Optional for Turso cloud databases
});
```

### Creating an Index
```javascript
await store.createIndex("my-collection", 1536);
```

### Adding Vectors with Metadata
```javascript
const vectors = [[0.1, 0.2, ...], [0.3, 0.4, ...]];
const metadata = [
  { text: "first document", category: "A" },
  { text: "second document", category: "B" }
];

await store.upsert("my-collection", vectors, metadata);
```

### Querying Similar Vectors
```javascript
const queryVector = [0.1, 0.2, ...];
const results = await store.query("my-collection", queryVector, 10, { category: "A" });
```

## Constructor Options
- **connectionUrl**: `string` - LibSQL database URL (e.g., ':memory:', 'file:local.db', or LibSQL-compatible string).
- **authToken**: `string` - Optional for Turso cloud databases.
- **syncUrl**: `string` - URL for database replication (Turso specific).
- **syncInterval**: `number` - Interval in milliseconds for database sync (Turso specific).

## API Methods
### `createIndex(indexName: string, dimension: number, metric?: 'cosine' | 'euclidean' | 'dotproduct')`
- Creates a new vector collection.

### `upsert(indexName: string, vectors: number[][], metadata?: Record<string, any>[], ids?: string[])`
- Adds or updates vectors and metadata in the index.

### `query(indexName: string, queryVector: number[], topK?: number, filter?: Filter, includeVector?: boolean, minScore?: number)`
- Searches for similar vectors with optional metadata filtering.

### `describeIndex(indexName: string)`
- Gets information about an index.

### `deleteIndex(indexName: string)`
- Deletes an index and all its data.

### `listIndexes()`
- Lists all vector indexes in the database.

### `truncateIndex(indexName: string)`
- Removes all vectors from an index while keeping the index structure.

## Response Types
### Query Result Format
```javascript
interface QueryResult {
  id: string;
  score: number;
  metadata: Record<string, any>;
  vector?: number[]; // Included if includeVector is true
}
```

## Error Handling
Common error cases include:
- Invalid index name format
- Invalid vector dimensions
- Table/index not found
- Database connection issues
- Transaction failures during upsert

### Example Error Handling
```javascript
try {
  await store.query("my-collection", queryVector);
} catch (error) {
  if (error.message.includes("Invalid index name format")) {
    console.error("Index name must start with a letter/underscore and contain only alphanumeric characters");
  } else if (error.message.includes("Table not found")) {
    console.error("The specified index does not exist");
  } else {
    console.error("Vector store error:", error.message);
  }
}
```

## Related
- Metadata Filters
- Last updated on February 20, 2025
- PgVector
- PineconeVector

----------------------------------------
Rag/Document
https://mastra.ai/docs/reference/rag/document
----------------------------------------

# MDocument Class Overview

The `MDocument` class is designed for processing documents in RAG (Retrieval-Augmented Generation) applications. It provides methods to create documents from various formats and to manipulate document chunks.

## Constructor

- **Parameters:**
  - `docs`: `Array<{ text: string, metadata?: Record<string, any> }>` - An array of document chunks containing text and optional metadata.
  - `type`: `'text' | 'html' | 'markdown' | 'json' | 'latex'` - Specifies the type of document content.

## Static Methods

### `fromText`
Creates a document from plain text.

```typescript
static fromText(text: string, metadata?: Record<string, any>): MDocument
```

### `fromHTML`
Creates a document from HTML content.

```typescript
static fromHTML(html: string, metadata?: Record<string, any>): MDocument
```

### `fromMarkdown`
Creates a document from Markdown content.

```typescript
static fromMarkdown(markdown: string, metadata?: Record<string, any>): MDocument
```

### `fromJSON`
Creates a document from JSON content.

```typescript
static fromJSON(json: string, metadata?: Record<string, any>): MDocument
```

## Instance Methods

### `chunk`
Splits the document into chunks and optionally extracts metadata.

```typescript
async chunk(params?: ChunkParams): Promise<Chunk[]>
```

### `getDocs`
Returns an array of processed document chunks.

```typescript
getDocs(): Chunk[]
```

### `getText`
Returns an array of text strings from chunks.

```typescript
getText(): string[]
```

### `getMetadata`
Returns an array of metadata objects from chunks.

```typescript
getMetadata(): Record<string, any>[]
```

### `extractMetadata`
Extracts metadata using specified extractors.

```typescript
async extractMetadata(params: ExtractParams): Promise<MDocument>
```

## Example Usage

```typescript
import { MDocument } from '@mastra/rag';

// Create document from text
const doc = MDocument.fromText('Your content here');

// Split into chunks with metadata extraction
const chunks = await doc.chunk({
  strategy: 'markdown',
  headers: [['#', 'title'], ['##', 'section']],
  extract: {
    fields: [
      { name: 'summary', description: 'A brief summary' },
      { name: 'keywords', description: 'Key terms' }
    ]
  }
});

// Get processed chunks
const docs = doc.getDocs();
const texts = doc.getText();
const metadata = doc.getMetadata();
```

Last updated on February 20, 2025.

----------------------------------------
Rag/Embeddings
https://mastra.ai/docs/reference/rag/embeddings
----------------------------------------

# AI SDK Embedding Functions

The AI SDK provides two primary functions for generating vector embeddings from text inputs: `embed` for single inputs and `embedMany` for multiple inputs. These embeddings facilitate similarity searches and Retrieval-Augmented Generation (RAG) workflows.

## 1. `embed()`

### Description
Generates a vector embedding for a single text input.

### Syntax
```javascript
import { embed } from 'ai';

const result = await embed({
  model: openai.embedding('text-embedding-3-small'),
  value: "Your text to embed",
  maxRetries: 2 // optional, defaults to 2
});
```

### Parameters
- **model**: `EmbeddingModel` - The embedding model to use (e.g., `openai.embedding('text-embedding-3-small')`).
- **value**: `string | Record<string, any>` - The text content or object to embed.
- **maxRetries?**: `number` - Maximum number of retries per embedding call (default is 2; set to 0 to disable).
- **abortSignal?**: `AbortSignal` - Optional signal to cancel the request.
- **headers?**: `Record<string, string>` - Additional HTTP headers for the request (only for HTTP-based providers).

### Return Value
- **embedding**: `number[]` - The embedding vector for the input.

## 2. `embedMany()`

### Description
Generates vector embeddings for multiple text inputs at once.

### Syntax
```javascript
import { embedMany } from 'ai';

const result = await embedMany({
  model: openai.embedding('text-embedding-3-small'),
  values: [
    "First text",
    "Second text",
    "Third text"
  ],
  maxRetries: 2 // optional, defaults to 2
});
```

### Parameters
- **model**: `EmbeddingModel` - The embedding model to use (e.g., `openai.embedding('text-embedding-3-small')`).
- **values**: `string[] | Record<string, any>[]` - Array of text content or objects to embed.
- **maxRetries?**: `number` - Maximum number of retries per embedding call (default is 2; set to 0 to disable).
- **abortSignal?**: `AbortSignal` - Optional signal to cancel the request.
- **headers?**: `Record<string, string>` - Additional HTTP headers for the request (only for HTTP-based providers).

### Return Value
- **embeddings**: `number[][]` - Array of embedding vectors corresponding to the input values.

## Example Usage

### Single Embedding
```javascript
import { embed } from 'ai';
import { openai } from '@ai-sdk/openai';

const singleResult = await embed({
  model: openai.embedding('text-embedding-3-small'),
  value: "What is the meaning of life?"
});
```

### Multiple Embeddings
```javascript
import { embedMany } from 'ai';
import { openai } from '@ai-sdk/openai';

const multipleResult = await embedMany({
  model: openai.embedding('text-embedding-3-small'),
  values: [
    "First question about life",
    "Second question about universe",
    "Third question about everything"
  ]
});
```

For further details on embeddings, refer to the **AI SDK Embeddings Overview**. 

_Last updated on February 20, 2025._

----------------------------------------
Rag/Extract Params
https://mastra.ai/docs/reference/rag/extract-params
----------------------------------------

### ExtractParams Overview

`ExtractParams` is a configuration tool for automatic metadata extraction from document chunks using LLM analysis.

### Example Code

```javascript
const doc = new Document(text);

const chunks = await doc.chunk({
  extract: {
    fields: [
      { 
        name: 'summary', 
        description: 'A 1-2 sentence summary of the main points' 
      },
      { 
        name: 'entities', 
        description: 'List of companies, people, and locations mentioned' 
      },
      {
        name: 'custom_field',
        description: 'Any other metadata you want to extract, guided by this description'
      }
    ],
    model: 'gpt-4o-mini' // Optional: specify a different model
  }
});
```

### Parameters

- **fields**: `Array<{ name: string, description: string }>`
  - An array of fields to extract from each chunk.
  
- **model**: `string` (optional, defaults to `gpt-3.5-turbo`)
  - OpenAI model to use for extraction.

### Common Field Types

- **summary**: Brief overview of chunk content.
- **keywords**: Key terms or concepts.
- **topics**: Main subjects discussed.
- **entities**: Named entities (people, places, organizations).
- **sentiment**: Emotional tone.
- **language**: Detected language.
- **timestamp**: Temporal references.
- **categories**: Content classification.

### Note

You can define any metadata fields you want to extract, making the fields flexible to suit your needs.

----------------------------------------
Rag/Graph Rag
https://mastra.ai/docs/reference/rag/graph-rag
----------------------------------------

# GraphRAG Class Documentation

## Overview
The `GraphRAG` class implements a graph-based approach for retrieval augmented generation (RAG). It constructs a knowledge graph from document chunks, where nodes represent documents and edges represent semantic relationships. This allows for both direct similarity matching and the discovery of related content through graph traversal.

## Basic Usage

```javascript
import { GraphRAG } from "@mastra/rag";

const graphRag = new GraphRAG({
  dimension: 1536, // Dimension of the embedding vectors
  threshold: 0.7   // Similarity threshold for creating edges (0-1)
});

// Create the graph from chunks and embeddings
graphRag.createGraph(documentChunks, embeddings);

// Query the graph with an embedding
const results = await graphRag.query({
  query: queryEmbedding,
  topK: 10,
  randomWalkSteps: 100,
  restartProb: 0.15
});
```

## Constructor Parameters
- **dimension**: `number` (default: 1536)  
  The dimension of the embedding vectors.
  
- **threshold**: `number` (default: 0.7)  
  The similarity threshold for creating edges between nodes (range: 0-1).

## Methods

### 1. createGraph
Creates a knowledge graph from document chunks and their embeddings.

```javascript
createGraph(chunks: GraphChunk[], embeddings: GraphEmbedding[]): void
```

**Parameters**:
- **chunks**: `GraphChunk[]`  
  An array of document chunks with text and metadata.
  
- **embeddings**: `GraphEmbedding[]`  
  An array of embeddings corresponding to the chunks.

### 2. query
Performs a graph-based search combining vector similarity and graph traversal.

```javascript
query({
  query: number[],
  topK?: number,
  randomWalkSteps?: number,
  restartProb?: number
}): RankedNode[]
```

**Parameters**:
- **query**: `number[]`  
  The query embedding vector.
  
- **topK**: `number` (default: 10)  
  The number of results to return.
  
- **randomWalkSteps**: `number` (default: 100)  
  The number of steps in the random walk.
  
- **restartProb**: `number` (default: 0.15)  
  The probability of restarting the walk from the query node.

**Returns**:  
An array of `RankedNode` objects, each containing:
- **id**: `string` - Unique identifier for the node.
- **content**: `string` - Text content of the document chunk.
- **metadata**: `Record<string, any>` - Additional metadata associated with the chunk.
- **score**: `number` - Combined relevance score from graph traversal.

## Advanced Example

```javascript
const graphRag = new GraphRAG({
  dimension: 1536,
  threshold: 0.8 // Stricter similarity threshold
});

// Create graph from chunks and embeddings
graphRag.createGraph(documentChunks, embeddings);

// Query with custom parameters
const results = await graphRag.query({
  query: queryEmbedding,
  topK: 5,
  randomWalkSteps: 200,
  restartProb: 0.2
});
```

## Related
- **createGraphRAGTool**

_Last updated on February 20, 2025_

----------------------------------------
Rag/Metadata Filters
https://mastra.ai/docs/reference/rag/metadata-filters
----------------------------------------

# Metadata Filters Documentation Summary

Mastra provides a unified metadata filtering syntax across all vector stores, utilizing a MongoDB/Sift query syntax. Each vector store translates these filters into their native format.

## Basic Example

```javascript
import { PgVector } from '@mastra/pg';

const store = new PgVector(connectionString);

const results = await store.query(
  "my_index",
  queryVector,
  10,
  {
    category: "electronics",  // Simple equality
    price: { $gt: 100 },      // Numeric comparison
    tags: { $in: ["sale", "new"] }  // Array membership
  }
);
```

## Supported Operators

### Basic Comparison Operators
| Operator | Description | Example |
|----------|-------------|---------|
| `$eq`    | Matches values equal to specified value | `{ age: { $eq: 25 } }` |
| `$ne`    | Matches values not equal | `{ status: { $ne: 'inactive' } }` |
| `$gt`    | Greater than | `{ price: { $gt: 100 } }` |
| `$gte`   | Greater than or equal | `{ rating: { $gte: 4.5 } }` |
| `$lt`    | Less than | `{ stock: { $lt: 20 } }` |
| `$lte`   | Less than or equal | `{ priority: { $lte: 3 } }` |

### Array Operators
| Operator | Description | Example |
|----------|-------------|---------|
| `$in`    | Matches any value in array | `{ category: { $in: ["A", "B"] } }` |
| `$nin`   | Matches none of the values | `{ status: { $nin: ["deleted", "archived"] } }` |
| `$all`   | Matches arrays containing all elements | `{ tags: { $all: ["urgent", "high"] } }` |
| `$elemMatch` | Matches array elements meeting criteria | `{ scores: { $elemMatch } }` |

### Logical Operators
| Operator | Description | Example |
|----------|-------------|---------|
| `$and`   | Logical AND | `{ $and: [ { price: { $gt: 100 } }, { stock: { $gt: 0 } } ] }` |
| `$or`    | Logical OR | `{ $or: [ { status: "active" }, { priority: "high" } ] }` |
| `$not`   | Logical NOT | `{ price: { $not } }` |
| `$nor`   | Logical NOR | `{ $nor: [ { status: "deleted" }, { archived: true } ] }` |

### Element Operators
| Operator | Description | Example |
|----------|-------------|---------|
| `$exists` | Matches documents with field | `{ rating: { $exists: true } }` |

### Custom Operators
| Operator | Description | Example |
|----------|-------------|---------|
| `$contains` | Text contains substring | `{ description: { $contains: "sale" } }` |
| `$regex` | Regular expression match | `{ name: { $regex: "^test" } }` |
| `$size` | Array length check | `{ tags: { $size } }` |
| `$geo` | Geospatial query | `{ location: { $geo } }` |
| `$datetime` | Datetime range query | `{ created: { $datetime } }` |
| `$hasId` | Vector ID existence check | `{ $hasId: [ "id1", "id2" ] }` |
| `$hasVector` | Vector existence check | `{ $hasVector: true }` |

## Common Rules and Restrictions
- **Field Names**: Cannot contain dots (.) unless referring to nested fields, cannot start with `$`, cannot be empty strings.
- **Values**: Must be valid JSON types, not undefined, and properly typed for the operator.
- **Logical Operators**: Must contain valid conditions, cannot be empty, and must be properly nested.

## Store-Specific Notes
- **Astra**: Supports nested field queries using dot notation.
- **ChromaDB**: Filters only return results where the filtered field exists.
- **Cloudflare Vectorize**: Requires explicit metadata indexing before filtering.
- **LibSQL**: Supports nested object queries with dot notation.
- **PgVector**: Full support for PostgreSQL’s native JSON querying capabilities.
- **Pinecone**: Metadata field names limited to 512 characters.
- **Qdrant**: Supports advanced filtering with nested conditions.
- **Upstash**: 512-character limit for metadata field keys.

This summary provides an overview of the metadata filtering capabilities in Mastra, including examples of usage and supported operators across different vector stores.

----------------------------------------
Rag/Pg
https://mastra.ai/docs/reference/rag/pg
----------------------------------------

## PgVector Class Overview

The `PgVector` class enables vector search functionality using PostgreSQL with the `pgvector` extension, allowing for efficient vector similarity searches within a PostgreSQL database.

### Constructor Options
- **connectionString**: `string` - PostgreSQL connection URL.

### Methods

1. **createIndex()**
   - **Parameters**:
     - `indexName`: `string` - Name of the index to create.
     - `dimension`: `number` - Vector dimension (must match your embedding model).
     - `metric?`: `'cosine' | 'euclidean' | 'dotproduct'` (default: `cosine`) - Distance metric for similarity search.

2. **upsert()**
   - **Parameters**:
     - `indexName`: `string` - Name of the index to upsert vectors into.
     - `vectors`: `number[][]` - Array of embedding vectors.
     - `metadata?`: `Record<string, any>[]` - Metadata for each vector.
     - `ids?`: `string[]` - Optional vector IDs (auto-generated if not provided).

3. **query()**
   - **Parameters**:
     - `indexName`: `string` - Name of the index to query.
     - `vector`: `number[]` - Query vector.
     - `topK?`: `number` (default: `10`) - Number of results to return.
     - `filter?`: `Record<string, any>` - Metadata filters.
     - `includeVector?`: `boolean` (default: `false`) - Whether to include the vector in the result.
     - `minScore?`: `number` (default: `0`) - Minimum similarity score threshold.

4. **listIndexes()**
   - **Returns**: An array of index names as strings.

5. **describeIndex()**
   - **Parameters**:
     - `indexName`: `string` - Name of the index to describe.
   - **Returns**: 
     ```typescript
     interface IndexStats {
       dimension: number;
       count: number;
       metric: "cosine" | "euclidean" | "dotproduct";
     }
     ```

6. **deleteIndex()**
   - **Parameters**:
     - `indexName`: `string` - Name of the index to delete.

7. **disconnect()**
   - Closes the database connection pool. Should be called when done using the store.

### Response Types
- **Query Results**:
  ```typescript
  interface QueryResult {
    id: string;
    score: number;
    metadata: Record<string, any>;
  }
  ```

### Error Handling
The store throws typed errors that can be caught:
```typescript
try {
  await store.query("index_name", queryVector);
} catch (error) {
  if (error instanceof VectorStoreError) {
    console.log(error.code); // e.g., 'connection_failed', 'invalid_dimension', etc.
    console.log(error.details); // Additional error context.
  }
}
```

### Related Topics
- Metadata Filters

**Last updated**: February 20, 2025

----------------------------------------
Rag/Pinecone
https://mastra.ai/docs/reference/rag/pinecone
----------------------------------------

# PineconeVector Class Documentation

The `PineconeVector` class interfaces with Pinecone's vector database, enabling real-time vector search with features such as hybrid search, metadata filtering, and namespace management.

## Constructor Options
- **apiKey**: `string` - Pinecone API key.
- **environment**: `string` - Pinecone environment (e.g., `"us-west1-gcp"`).

## Methods

### createIndex()
Creates a new index in Pinecone.
- **Parameters**:
  - **indexName**: `string` - Name of the index to create.
  - **dimension**: `number` - Vector dimension (must match your embedding model).
  - **metric**: `'cosine' | 'euclidean' | 'dotproduct'` (default: `cosine`) - Distance metric for similarity search.

### upsert()
Inserts or updates vectors in the specified index.
- **Parameters**:
  - **indexName**: `string` - Name of your Pinecone index.
  - **vectors**: `number[][]` - Array of embedding vectors.
  - **metadata**: `Record<string, any>[]` (optional) - Metadata for each vector.
  - **ids**: `string[]` (optional) - Vector IDs (auto-generated if not provided).

### query()
Queries the index for similar vectors.
- **Parameters**:
  - **indexName**: `string` - Name of the index to query.
  - **vector**: `number[]` - Query vector to find similar vectors.
  - **topK**: `number` (default: `10`) - Number of results to return.
  - **filter**: `Record<string, any>` (optional) - Metadata filters for the query.
  - **includeVector**: `boolean` (default: `false`) - Whether to include the vector in the result.

### listIndexes()
Returns an array of index names as strings.

### describeIndex()
Describes the specified index.
- **Parameters**:
  - **indexName**: `string` - Name of the index to describe.
- **Returns**: 
  - `IndexStats` interface containing:
    - **dimension**: `number`
    - **count**: `number`
    - **metric**: `'cosine' | 'euclidean' | 'dotproduct'`

### deleteIndex()
Deletes the specified index.
- **Parameters**:
  - **indexName**: `string` - Name of the index to delete.

## Response Types

### Query Results
Returned in the following format:
- `QueryResult` interface:
  - **id**: `string`
  - **score**: `number`
  - **metadata**: `Record<string, any>`

## Error Handling
Typed errors can be caught during operations:
```javascript
try {
  await store.query("index_name", queryVector);
} catch (error) {
  if (error instanceof VectorStoreError) {
    console.log(error.code); // e.g., 'connection_failed', 'invalid_dimension'
    console.log(error.details); // Additional error context
  }
}
```

## Environment Variables
Required:
- **PINECONE_API_KEY**: Your Pinecone API key.
- **PINECONE_ENVIRONMENT**: Pinecone environment (e.g., ‘us-west1-gcp’).

## Related Topics
- Metadata Filters

_Last updated on February 20, 2025_

----------------------------------------
Rag/Qdrant
https://mastra.ai/docs/reference/rag/qdrant
----------------------------------------

# QdrantVector Class Documentation

The `QdrantVector` class facilitates vector search using Qdrant, a vector similarity search engine. It offers a production-ready service with an API to store, search, and manage vectors, including support for additional payloads and extended filtering.

## Constructor Options

- **url**: `string`  
  REST URL of the Qdrant instance (e.g., `https://xyz-example.eu-central.aws.cloud.qdrant.io:6333`).
  
- **apiKey**: `string` (optional)  
  Qdrant API key.
  
- **https**: `boolean`  
  Indicates whether to use TLS for the connection (recommended).

## Methods

### 1. `createIndex()`

- **Parameters**:
  - **indexName**: `string` - Name of the index to create.
  - **dimension**: `number` - Vector dimension (must match your embedding model).
  - **metric**: `'cosine' | 'euclidean' | 'dotproduct'` (default: `cosine`) - Distance metric for similarity search.

### 2. `upsert()`

- **Parameters**:
  - **vectors**: `number[][]` - Array of embedding vectors.
  - **metadata**: `Record<string, any>[]` (optional) - Metadata for each vector.
  - **ids**: `string[]` (optional) - Vector IDs (auto-generated if not provided).

### 3. `query()`

- **Parameters**:
  - **indexName**: `string` - Name of the index to query.
  - **queryVector**: `number[]` - Query vector to find similar vectors.
  - **topK**: `number` (default: `10`) - Number of results to return.
  - **filter**: `Record<string, any>` (optional) - Metadata filters for the query.
  - **includeVector**: `boolean` (default: `false`) - Whether to include vectors in the results.

### 4. `listIndexes()`

- **Returns**: An array of index names as strings.

### 5. `describeIndex()`

- **Parameters**:
  - **indexName**: `string` - Name of the index to describe.

- **Returns**: 
  ```typescript
  interface IndexStats {
      dimension: number;
      count: number;
      metric: "cosine" | "euclidean" | "dotproduct";
  }
  ```

### 6. `deleteIndex()`

- **Parameters**:
  - **indexName**: `string` - Name of the index to delete.

## Response Types

### Query Results

Returned in the following format:
```typescript
interface QueryResult {
    id: string;
    score: number;
    metadata: Record<string, any>;
}
```

## Error Handling

Typed errors can be caught as follows:
```javascript
try {
    await store.query("index_name", queryVector);
} catch (error) {
    if (error instanceof VectorStoreError) {
        console.log(error.code); // 'connection_failed' | 'invalid_dimension' | etc.
        console.log(error.details); // Additional error context.
    }
}
```

## Related Classes

- **Metadata Filters**
- **PineconeVector**
- **UpstashVector**

_Last updated on February 20, 2025._

----------------------------------------
Rag/Rerank
https://mastra.ai/docs/reference/rag/rerank
----------------------------------------

### Rerank Function Overview

The `rerank()` function enhances vector search results by integrating semantic relevance, vector similarity, and position-based scoring.

#### Function Signature

```typescript
rerank(
  results: QueryResult[],
  query: string,
  modelConfig: ModelConfig,
  options?: RerankerFunctionOptions
): Promise<RerankResult[]>
```

#### Parameters

- **results**: `QueryResult[]`  
  The vector search results to be reranked.

- **query**: `string`  
  The search query text for evaluating relevance.

- **model**: `LanguageModelV1`  
  The language model used for reranking.

- **options**: `RerankerFunctionOptions` (optional)  
  Configuration options for the reranking model.

#### RerankerFunctionOptions

- **weights**: `WeightConfig` (optional)  
  Weights for scoring components (must total 1).
  - **semantic**: `number` (default: 0.4) - Weight for semantic relevance.
  - **vector**: `number` (default: 0.4) - Weight for vector similarity.
  - **position**: `number` (default: 0.2) - Weight for position-based scoring.
  
- **queryEmbedding**: `number[]` (optional)  
  Embedding of the query.

- **topK**: `number` (default: 3)  
  Number of top results to return.

#### Returns

The function returns a `Promise` that resolves to an array of `RerankResult` objects:

- **result**: `QueryResult`  
  The original query result.

- **score**: `number`  
  Combined reranking score (0-1).

- **details**: `ScoringDetails`  
  Detailed scoring information.

#### ScoringDetails

- **semantic**: `number` (0-1)  
  Semantic relevance score.

- **vector**: `number` (0-1)  
  Vector similarity score.

- **position**: `number` (0-1)  
  Position-based score.

- **queryAnalysis**: `object` (optional)  
  Details about query analysis.
  - **magnitude**: `number`  
    Magnitude of the query.
  - **dominantFeatures**: `number[]`  
    Dominant features of the query.

#### Usage Example

```javascript
import { openai } from "@ai-sdk/openai";
import { rerank } from "@mastra/rag";

const model = openai("gpt-4o-mini");

const rerankedResults = await rerank(
  vectorSearchResults,
  "How do I deploy to production?",
  model,
  {
    weights: {
      semantic: 0.5,
      vector: 0.3,
      position: 0.2
    },
    topK: 3
  }
);
```

### Additional Notes

- The `rerank` function can utilize any LanguageModel from the Vercel AI SDK. If using the Cohere model `rerank-v3.5`, it will automatically leverage Cohere’s reranking capabilities.

### Last Updated

February 20, 2025

----------------------------------------
Rag/Upstash
https://mastra.ai/docs/reference/rag/upstash
----------------------------------------

# UpstashVector Class Documentation

The `UpstashVector` class provides a serverless vector database service for vector similarity search with metadata filtering capabilities.

## Constructor Options
- **url**: `string` - Upstash Vector database URL.
- **token**: `string` - Upstash Vector API token.

## Methods

### 1. createIndex()
- **Parameters**:
  - **indexName**: `string` - Name of the index to create.
  - **dimension**: `number` - Vector dimension (must match your embedding model).
  - **metric**: `'cosine' | 'euclidean' | 'dotproduct'` (default: `cosine`) - Distance metric for similarity search.
- **Note**: This method is a no-op for Upstash as indexes are created automatically.

### 2. upsert()
- **Parameters**:
  - **indexName**: `string` - Name of the index to upsert into.
  - **vectors**: `number[][]` - Array of embedding vectors.
  - **metadata**: `Record<string, any>[]` (optional) - Metadata for each vector.
  - **ids**: `string[]` (optional) - Optional vector IDs (auto-generated if not provided).

### 3. query()
- **Parameters**:
  - **indexName**: `string` - Name of the index to query.
  - **queryVector**: `number[]` - Query vector to find similar vectors.
  - **topK**: `number` (default: `10`) - Number of results to return.
  - **filter**: `Record<string, any>` (optional) - Metadata filters for the query.
  - **includeVector**: `boolean` (default: `false`) - Whether to include vectors in the results.

### 4. listIndexes()
- **Returns**: An array of index names (namespaces) as strings.

### 5. describeIndex()
- **Parameters**:
  - **indexName**: `string` - Name of the index to describe.
- **Returns**: An object of type `IndexStats`:
  ```typescript
  interface IndexStats {
      dimension: number;
      count: number;
      metric: "cosine" | "euclidean" | "dotproduct";
  }
  ```

### 6. deleteIndex()
- **Parameters**:
  - **indexName**: `string` - Name of the index (namespace) to delete.

## Response Types

### Query Results
- **Returns**: An object of type `QueryResult`:
  ```typescript
  interface QueryResult {
      id: string;
      score: number;
      metadata: Record<string, any>;
  }
  ```

## Error Handling
The store throws typed errors that can be caught:
```typescript
try {
    await store.query("index_name", queryVector);
} catch (error) {
    if (error instanceof VectorStoreError) {
        console.log(error.code); // 'connection_failed' | 'invalid_dimension' | etc.
        console.log(error.details); // Additional error context.
    }
}
```

## Environment Variables
### Required
- **UPSTASH_VECTOR_URL**: Your Upstash Vector database URL.
- **UPSTASH_VECTOR_TOKEN**: Your Upstash Vector API token.

## Related
- Metadata Filters

_Last updated on February 20, 2025_

----------------------------------------
Rag/Vectorize
https://mastra.ai/docs/reference/rag/vectorize
----------------------------------------

# Cloudflare Vector Store Documentation Summary

## Overview
The `CloudflareVector` class enables vector search using Cloudflare Vectorize, a vector database service integrated with Cloudflare’s edge network.

## Constructor Options
- **accountId**: `string` - Cloudflare account ID.
- **apiToken**: `string` - Cloudflare API token with Vectorize permissions.

## Methods

### 1. `createIndex()`
- **Parameters**:
  - `indexName`: `string` - Name of the index to create.
  - `dimension`: `number` - Vector dimension (must match your embedding model).
  - `metric?`: `'cosine' | 'euclidean' | 'dotproduct'` (default: `cosine`) - Distance metric for similarity search.

### 2. `upsert()`
- **Parameters**:
  - `indexName`: `string` - Name of the index to upsert into.
  - `vectors`: `number[][]` - Array of embedding vectors.
  - `metadata?`: `Record<string, any>[]` - Metadata for each vector.
  - `ids?`: `string[]` - Optional vector IDs (auto-generated if not provided).

### 3. `query()`
- **Parameters**:
  - `indexName`: `string` - Name of the index to query.
  - `queryVector`: `number[]` - Query vector to find similar vectors.
  - `topK?`: `number` (default: `10`) - Number of results to return.
  - `filter?`: `Record<string, any>` - Metadata filters for the query.
  - `includeVector?`: `boolean` (default: `false`) - Whether to include vectors in the results.

### 4. `listIndexes()`
- **Returns**: An array of index names as strings.

### 5. `describeIndex()`
- **Parameters**:
  - `indexName`: `string` - Name of the index to describe.
- **Returns**: `IndexStats` interface containing:
  - `dimension`: `number`
  - `count`: `number`
  - `metric`: `"cosine" | "euclidean" | "dotproduct"`

### 6. `deleteIndex()`
- **Parameters**:
  - `indexName`: `string` - Name of the index to delete.

### 7. `createMetadataIndex()`
- **Parameters**:
  - `indexName`: `string` - Name of the index containing the metadata field.
  - `propertyName`: `string` - Name of the metadata field to index.
  - `indexType`: `'string' | 'number' | 'boolean'` - Type of the metadata field.

### 8. `deleteMetadataIndex()`
- **Parameters**:
  - `indexName`: `string` - Name of the index containing the metadata field.
  - `propertyName`: `string` - Name of the metadata field to remove indexing from.

### 9. `listMetadataIndexes()`
- **Parameters**:
  - `indexName`: `string` - Name of the index to list metadata indexes for.

## Response Types
- **Query Result Format**:
```typescript
interface QueryResult {
  id: string;
  score: number;
  metadata: Record<string, any>;
}
```

## Error Handling
The store throws typed errors that can be caught:
```javascript
try {
  await store.query("index_name", queryVector);
} catch (error) {
  if (error instanceof VectorStoreError) {
    console.log(error.code); // 'connection_failed' | 'invalid_dimension' | etc.
    console.log(error.details); // Additional error context.
  }
}
```

## Environment Variables
Required:
- **CLOUDFLARE_ACCOUNT_ID**: Your Cloudflare account ID.
- **CLOUDFLARE_API_TOKEN**: Your Cloudflare API token with Vectorize permissions.

## Related Topics
- Metadata Filters

_Last updated on February 20, 2025_

----------------------------------------
Storage/Libsql
https://mastra.ai/docs/reference/storage/libsql
----------------------------------------

# LibSQL Storage Documentation Summary

## Overview
LibSQL Storage provides a SQLite-compatible storage solution that can operate in-memory or as a persistent database.

## Installation
To install the LibSQL storage library, use the following npm command:
```bash
npm install @mastra/storage-libsql
```

## Usage
To use LibSQL Storage, import the `DefaultStorage` class from the library and initialize it with the desired configuration.

### Example Code

#### In-Memory Database (Development)
```javascript
import { DefaultStorage } from "@mastra/core/storage";

const storage = new DefaultStorage({
  url: ":memory:"
});
```

#### Persistent Database (Production)
```javascript
import { DefaultStorage } from "@mastra/core/storage";

const storage = new DefaultStorage({
  url: process.env.DATABASE_URL
});
```

## Parameters
- **url**: `string`  
  - Database URL. Use `':memory:'` for in-memory storage or a LibSQL-compatible connection string for persistent storage.
  
- **authToken**: `string` (optional)  
  - Authentication token for remote LibSQL databases.

## Additional Notes

### In-Memory vs Persistent Storage
- **In-Memory Storage** (`:memory:`):
  - Suitable for development, testing, and quick prototyping.
  - Data does not persist after the application is stopped.

- **Persistent Storage**:
  - Use a persistent database URL for production:
    - Local file: `file:local.db`
    - Remote LibSQL: `libsql://your-database.turso.io`

### Schema Management
The storage implementation automatically handles schema creation and updates, creating the following tables:
- **threads**: Stores conversation threads.
- **messages**: Stores individual messages.
- **metadata**: Stores additional metadata for threads and messages.

_Last updated on February 20, 2025._

----------------------------------------
Storage/Postgresql
https://mastra.ai/docs/reference/storage/postgresql
----------------------------------------

# PostgreSQL Storage Documentation

## Overview
The PostgreSQL storage implementation offers a production-ready solution for storing data using PostgreSQL databases.

## Installation
To install the PostgreSQL storage package, use the following npm command:
```bash
npm install @mastra/pg
```

## Usage
To use the PostgreSQL storage, import the `PostgresStore` class and create an instance with the required parameters:
```javascript
import { PostgresStore } from "@mastra/pg";

const storage = new PostgresStore({
  connectionString: process.env.DATABASE_URL,
});
```

## Parameters
- **connectionString**: `string`
  - Description: PostgreSQL connection string.
  - Example: `postgresql://user:pass@host:5432/dbname`

## Schema Management
The storage implementation automatically manages schema creation and updates, creating the following tables:
- **threads**: Stores conversation threads.
- **messages**: Stores individual messages.
- **metadata**: Stores additional metadata for threads and messages.

_Last updated on February 20, 2025_

----------------------------------------
Storage/Upstash
https://mastra.ai/docs/reference/storage/upstash
----------------------------------------

# Upstash Storage Documentation Summary

## Overview
Upstash Storage offers a serverless-friendly solution utilizing a Redis-compatible key-value store.

## Installation
To install Upstash Storage, use the following command:
```bash
npm install @mastra/upstash
```

## Usage
To use Upstash Storage, import and initialize `UpstashStore` as follows:
```javascript
import { UpstashStore } from "@mastra/upstash";

const storage = new UpstashStore({
  url: process.env.UPSTASH_URL,
  token: process.env.UPSTASH_TOKEN,
});
```

### Parameters
- **url**: `string` - Upstash Redis URL.
- **token**: `string` - Upstash Redis authentication token.
- **prefix**: `string` (optional, default: `mastra`) - Key prefix for all stored items.

## Key Structure
The storage implementation uses a specific key format:
- **Thread keys**: `{prefix}thread:{threadId}`
- **Message keys**: `{prefix}message:{messageId}`
- **Metadata keys**: `{prefix}metadata:{entityId}`

## Serverless Benefits
Upstash Storage is optimized for serverless environments:
- No connection management required.
- Pay-per-request pricing model.
- Global replication options available.
- Edge-compatible.

## Data Persistence Features
Upstash provides:
- Automatic data persistence.
- Point-in-time recovery.
- Cross-region replication options.

## Performance Considerations
For optimal performance:
- Use appropriate key prefixes to organize data.
- Monitor Redis memory usage.
- Implement data expiration policies as needed.

**Last updated on**: February 20, 2025.

----------------------------------------
Tools/Client
https://mastra.ai/docs/reference/tools/client
----------------------------------------

# MastraMCPClient Documentation Summary

## Overview
The `MastraMCPClient` class is designed for interacting with Model Context Protocol (MCP) servers, managing connections, discovering resources, and executing tools via the MCP protocol.

## Constructor
Creates a new instance of `MastraMCPClient`.

### Syntax
```javascript
new MastraMCPClient({
    name: string,
    version?: string = '1.0.0',
    server: StdioServerParameters | SSEClientParameters,
    capabilities?: ClientCapabilities = {},
})
```

### Parameters
- **name**: `string` - Identifier for the client instance.
- **version**: `string` (optional, default: '1.0.0') - Version of the client.
- **server**: `StdioServerParameters | SSEClientParameters` - Configuration for stdio or SSE server connection.
- **capabilities**: `ClientCapabilities` (optional, default: {}) - Client capabilities configuration.

## Methods
- **connect()**
  - Establishes a connection with the MCP server.
  - **Returns**: `Promise<void>`

- **disconnect()**
  - Closes the connection with the MCP server.
  - **Returns**: `Promise<void>`

- **resources()**
  - Retrieves the list of available resources from the server.
  - **Returns**: `Promise<ListResourcesResult>`

- **tools()**
  - Fetches and initializes available tools from the server, returning them in Mastra-compatible formats.
  - **Returns**: `Promise<Record<string, Tool>>`

## Examples

### Example with Stdio Server
```javascript
import { Agent } from "@mastra/core/agent";
import { MastraMCPClient } from "@mastra/mcp";
import { openai } from "@ai-sdk/openai";

// Initialize the MCP client
const fetchClient = new MastraMCPClient({
    name: "fetch",
    server: {
        command: "docker",
        args: ["run", "-i", "--rm", "mcp/fetch"],
    },
});

// Create a Mastra Agent
const agent = new Agent({
    name: "Fetch agent",
    instructions: "You are able to fetch data from URLs on demand and discuss the response data with the user.",
    model: openai("gpt-4o-mini"),
});

try {
    // Connect to the MCP server
    await fetchClient.connect();

    // Handle process exits
    process.on("exit", () => {
        fetchClient.disconnect();
    });

    // Get available tools
    const tools = await fetchClient.tools();

    // Use the agent with the MCP tools
    const response = await agent.generate("Tell me about mastra.ai/docs.", {
        toolsets: {
            fetch: tools,
        },
    });

    console.log("\n\n" + response.text);
} catch (error) {
    console.error("Error:", error);
} finally {
    // Always disconnect when done
    await fetchClient.disconnect();
}
```

### Example with SSE Server
```javascript
// Initialize the MCP client using an SSE server
const sseClient = new MastraMCPClient({
    name: "sse-client",
    server: {
        url: new URL("https://your-mcp-server.com/sse"),
        requestInit: {
            headers: {
                "Authorization": "Bearer your-token",
            },
        },
    },
});

// The rest of the usage is identical to the stdio example
```

## Related Information
For more details about the Model Context Protocol, refer to the `@modelcontextprotocol/sdk` documentation.

_Last updated on February 20, 2025_

----------------------------------------
Tools/Document Chunker Tool
https://mastra.ai/docs/reference/tools/document-chunker-tool
----------------------------------------

### createDocumentChunkerTool()

The `createDocumentChunkerTool()` function is designed to split documents into smaller, manageable chunks for efficient processing and retrieval. It allows for various chunking strategies and configurable parameters.

#### Basic Usage

```javascript
import { createDocumentChunkerTool, MDocument } from "@mastra/rag";

const document = new MDocument({
  text: "Your document content here...",
  metadata: { source: "user-manual" }
});

const chunker = createDocumentChunkerTool({
  doc: document,
  params: {
    strategy: "recursive",
    size: 512,
    overlap: 50,
    separator: "\n"
  }
});

const { chunks } = await chunker.execute();
```

#### Parameters

- **doc**: `MDocument`  
  The document to be chunked.

- **params**: `ChunkParams` (optional)  
  Configuration parameters for chunking.

##### ChunkParams

- **strategy**: `'recursive'` (default: `'recursive'`)  
  The chunking strategy to use.

- **size**: `number` (default: `512`)  
  Target size of each chunk in tokens/characters.

- **overlap**: `number` (default: `50`)  
  Number of overlapping tokens/characters between chunks.

- **separator**: `string` (default: `'\n'`)  
  Character(s) to use as chunk separator.

#### Returns

- **chunks**: `DocumentChunk[]`  
  An array of document chunks containing their content and metadata.

#### Example with Custom Parameters

```javascript
const technicalDoc = new MDocument({
  text: longDocumentContent,
  metadata: {
    type: "technical",
    version: "1.0"
  }
});

const chunker = createDocumentChunkerTool({
  doc: technicalDoc,
  params: {
    strategy: "recursive",
    size: 1024,      // Larger chunks
    overlap: 100,    // More overlap
    separator: "\n\n" // Split on double newlines
  }
});

const { chunks } = await chunker.execute();

// Process the chunks
chunks.forEach((chunk, index) => {
  console.log(`Chunk ${index + 1} length: ${chunk.content.length}`);
});
```

#### Tool Details

- **Tool ID**: Document Chunker {strategy} {size}
- **Description**: Chunks document using {strategy} strategy with size {size} and {overlap} overlap.
- **Input Schema**: Empty object (no additional inputs required).
- **Output Schema**: Object containing the chunks array.

#### Related

- **MDocument**
- **createVectorQueryTool**
- **createGraphRAGTool** 

_Last updated on February 20, 2025._

----------------------------------------
Tools/Graph Rag Tool
https://mastra.ai/docs/reference/tools/graph-rag-tool
----------------------------------------

### createGraphRAGTool()

The `createGraphRAGTool()` function creates a tool that enhances Retrieval-Augmented Generation (RAG) by constructing a graph of semantic relationships among documents. It utilizes the GraphRAG system for graph-based retrieval, which identifies relevant content through both direct similarity and interconnected relationships.

#### Usage Example

```javascript
import { openai } from "@ai-sdk/openai";
import { createGraphRAGTool } from "@mastra/rag";

const graphTool = createGraphRAGTool({
  vectorStoreName: "pinecone",
  indexName: "docs",
  model: openai.embedding('text-embedding-3-small'),
  graphOptions: {
    dimension: 1536,
    threshold: 0.7,
    randomWalkSteps: 100,
    restartProb: 0.15
  }
});
```

#### Parameters

- **vectorStoreName**: `string`  
  Name of the vector store to query.

- **indexName**: `string`  
  Name of the index within the vector store.

- **model**: `EmbeddingModel`  
  Embedding model to use for vector search.

- **graphOptions**: `GraphOptions` (optional)  
  Configuration for graph-based retrieval.

##### GraphOptions

- **dimension**: `number` (default: 1536)  
  Dimension of the embedding vectors.

- **threshold**: `number` (default: 0.7)  
  Similarity threshold for creating edges between nodes (range: 0-1).

- **randomWalkSteps**: `number` (default: 100)  
  Number of steps in random walk for graph traversal.

- **restartProb**: `number` (default: 0.15)  
  Probability of restarting the random walk from the query node.

#### Returns

The tool returns an object containing:

- **relevantContext**: `string`  
  Combined text from the most relevant document chunks, retrieved using graph-based ranking.

#### Advanced Example

```javascript
const graphTool = createGraphRAGTool({
  vectorStoreName: "pinecone",
  indexName: "docs",
  model: openai.embedding('text-embedding-3-small'),
  graphOptions: {
    dimension: 1536,
    threshold: 0.8,         // Higher similarity threshold
    randomWalkSteps: 200,   // More exploration steps
    restartProb: 0.2        // Higher restart probability
  }
});
```

### Related Functions

- `createVectorQueryTool()`
- `createDocumentChunkerTool()`
- `GraphRAG`

_Last updated on February 20, 2025_

----------------------------------------
Tools/Vector Query Tool
https://mastra.ai/docs/reference/tools/vector-query-tool
----------------------------------------

### createVectorQueryTool()

The `createVectorQueryTool()` function is designed for semantic search over vector stores, enabling filtering, reranking, and integration with various vector store backends.

#### Basic Usage

```javascript
import { openai } from '@ai-sdk/openai';
import { createVectorQueryTool } from "@mastra/rag";

const queryTool = createVectorQueryTool({
  vectorStoreName: "pinecone",
  indexName: "docs",
  model: openai.embedding('text-embedding-3-small'),
});
```

#### Parameters

- **vectorStoreName**: `string` - Name of the vector store (must be configured in Mastra).
- **indexName**: `string` - Name of the index within the vector store.
- **model**: `EmbeddingModel` - Embedding model for vector search.
- **reranker**: `RerankConfig` (optional) - Options for reranking results.
- **id**: `string` (optional) - Custom ID for the tool (default: `'VectorQuery {vectorStoreName} {indexName} Tool'`).
- **description**: `string` (optional) - Custom description for the tool.

#### RerankConfig

- **model**: `LanguageModelV1` - Language model for reranking.
- **options**: `RerankerOptions` (optional) - Options for the reranking process.
- **weights**: `WeightConfig` (optional) - Weights for scoring components (default: semantic: 0.4, vector: 0.4, position: 0.2).
- **topK**: `number` (optional) - Number of top results to return.

#### Returns

The tool returns an object containing:
- **relevantContext**: `string` - Combined text from the most relevant document chunks.

### Default Tool Description

The default description includes:
- Instructions for querying the vector store.
- Examples of filter syntax when filtering is enabled.

### Result Handling

The tool defaults to returning 10 results, adjustable based on query requirements.

### Example with Filters

```javascript
const queryTool = createVectorQueryTool({
  vectorStoreName: "pinecone",
  indexName: "docs",
  model: openai.embedding('text-embedding-3-small'),
  enableFilters: true,
});
```

With filtering enabled, the tool processes queries to create metadata filters. For example, a query like “Find content where the ‘version’ field is greater than 2.0” results in:

```json
{
  "version": { "$gt": 2.0 }
}
```

### Example with Reranking

```javascript
const queryTool = createVectorQueryTool({
  vectorStoreName: "milvus",
  indexName: "documentation",
  model: openai.embedding('text-embedding-3-small'),
  reranker: {
    model: openai('gpt-4o-mini'),
    options: {
      weights: {
        semantic: 0.5,
        vector: 0.3,
        position: 0.2
      },
      topK: 5
    }
  }
});
```

Reranking enhances result quality by combining:
- Semantic relevance (LLM-based scoring).
- Vector similarity (original vector distance scores).
- Position bias (consideration of original ordering).

### Tool Details

- **ID**: `VectorQuery {vectorStoreName} {indexName} Tool`
- **Input Schema**: Requires `queryText` and `filter` objects.
- **Output Schema**: Returns `relevantContext` string.

### Related Functions

- `rerank()`
- `createGraphRAGTool()`

----------------------------------------
Tts/Generate
https://mastra.ai/docs/reference/tts/generate
----------------------------------------

# TTS API Documentation

## Method: `.generate()`

The `.generate()` method is used to produce an audio response from the TTS model.

### Parameters
- **text**: `string`
  - The message to be processed by TTS.
- **voice**: `string`
  - Voice ID to be used for audio generation.

### Returns
- **audioResult**: `Readable`
  - The generated audio stream.

## Examples

### Basic Audio Generation (ElevenLabs)
```javascript
import { ElevenLabsTTS } from "@mastra/speech-elevenlabs";

const tts = new ElevenLabsTTS({
  model: {
    name: "eleven_multilingual_v2",
    apiKey: process.env.ELEVENLABS_API_KEY,
  },
});

const voices = await tts.voices();
const voiceId = voices?.[0]?.voice_id;

const { audioResult } = await tts.generate({
  text: "What is AI?",
  voice: voiceId,
});

await writeFile(path.join(process.cwd(), "/test-outputs/generate-output.mp3"), audioResult);
```

### Basic Audio Generation (OpenAI)
```javascript
import { OpenAITTS } from "@mastra/speech-openai";

const tts = new OpenAITTS({
  model: {
    name: "tts-1",
    apiKey: process.env.OPENAI_API_KEY,
  },
});

const voices = await tts.voices();
const voiceId = voices?.[0]?.voice_id;

const { audioResult } = await tts.generate({
  text: "What is AI?",
  voice: voiceId,
});

const outputPath = path.join(process.cwd(), "test-outputs/open-aigenerate-test.mp3");
writeFileSync(outputPath, audioResult);
```

### Basic Audio Generation (PlayAI)
```javascript
import { PlayAITTS } from "@mastra/speech-playai";

const tts = new PlayAITTS({
  model: {
    name: "PlayDialog",
    apiKey: process.env.PLAYAI_API_KEY,
  },
  userId: process.env.PLAYAI_USER_ID,
});

const voices = await tts.voices();
const voiceId = voices?.[0]?.voice_id;

const { audioResult } = await tts.generate({
  text: "What is AI?",
  voice: voiceId,
});

const outputPath = path.join(process.cwd(), "test-outputs/open-aigenerate-test.mp3");
writeFileSync(outputPath, audioResult);
```

### Azure Generation
```javascript
import { AzureTTS } from "@mastra/speech-azure";

const tts = new AzureTTS({
  model: {
    name: "en-US-JennyNeural",
    apiKey: process.env.AZURE_API_KEY,
    region: process.env.AZURE_REGION,
  },
});

const { audioResult } = await tts.generate({ text: "What is AI?" });
await writeFile(path.join(process.cwd(), "/test-outputs/azure-output.mp3"), audioResult);
```

### Deepgram Generation
```javascript
import { DeepgramTTS } from "@mastra/speech-deepgram";

const tts = new DeepgramTTS({
  model: {
    name: "aura",
    voice: "asteria-en",
    apiKey: process.env.DEEPGRAM_API_KEY,
  },
});

const { audioResult } = await tts.generate({ text: "What is AI?" });
await writeFile(path.join(process.cwd(), "/test-outputs/deepgram-output.mp3"), audioResult);
```

### Google Generation
```javascript
import { GoogleTTS } from "@mastra/speech-google";

const tts = new GoogleTTS({
  model: {
    name: "en-US-Standard-A",
    credentials: process.env.GOOGLE_CREDENTIALS,
  },
});

const { audioResult } = await tts.generate({ text: "What is AI?" });
await writeFile(path.join(process.cwd(), "/test-outputs/google-output.mp3"), audioResult);
```

### IBM Generation
```javascript
import { IbmTTS } from "@mastra/speech-ibm";

const tts = new IbmTTS({
  model: {
    voice: "en-US_AllisonV3Voice",
    apiKey: process.env.IBM_API_KEY,
  },
});

const { audioResult } = await tts.generate({ text: "What is AI?" });
await writeFile(path.join(process.cwd(), "/test-outputs/ibm-output.mp3"), audioResult);
```

### Murf Generation
```javascript
import { MurfTTS } from "@mastra/speech-murf";

const tts = new MurfTTS({
  model: {
    name: "GEN2",
    voice: "en-US-natalie",
    apiKey: process.env.MURF_API_KEY,
  },
});

const { audioResult } = await tts.generate({ text: "What is AI?" });
await writeFile(path.join(process.cwd(), "/test-outputs/murf-output.mp3"), audioResult);
```

## Related Methods
- For streaming audio responses, see the `.stream()` method documentation. 

_Last updated on February 20, 2025_

----------------------------------------
Tts/Providers And Models
https://mastra.ai/docs/reference/tts/providers-and-models
----------------------------------------

# Text-to-Speech (TTS) Providers and Models

## Popular Providers and Supported Models

1. **ElevenLabs**
   - Models: `eleven_multilingual_v2`, `eleven_flash_v2_5`, `eleven_flash_v2`, `eleven_multilingual_sts_v2`, `eleven_english_sts_v2`

2. **OpenAI**
   - Models: `tts-1`, `tts-1-hd`

3. **PlayAI**
   - Models: `PlayDialog`, `Play3.0-mini`

4. **Azure**
   - Various voices available through Azure Cognitive Services

5. **Deepgram**
   - Models: `aura`, `asteria-en`

6. **Google**
   - Various voices through Google Cloud Text-to-Speech

7. **IBM**
   - Models: `en-US_AllisonV3Voice`

8. **Murf**
   - Models: `GEN1`, `GEN2`, with voices like `en-US-natalie`

## Configuration Examples

### ElevenLabs Configuration
```javascript
import { ElevenLabsTTS } from "@mastra/speech-elevenlabs";

const tts = new ElevenLabsTTS({
  model: {
    name: "eleven_multilingual_v2",
    apiKey: process.env.ELEVENLABS_API_KEY,
  },
});
```

### OpenAI Configuration
```javascript
import { OpenAITTS } from "@mastra/speech-openai";

const tts = new OpenAITTS({
  model: {
    name: "tts-1", // or 'tts-1-hd' for higher quality
    apiKey: process.env.OPENAI_API_KEY,
  },
});
```

### PlayAI Configuration
```javascript
import { PlayAITTS } from "@mastra/speech-playai";

const tts = new PlayAITTS({
  model: {
    name: "PlayDialog", // or 'Play3.0-mini'
    apiKey: process.env.PLAYAI_API_KEY,
  },
  userId: process.env.PLAYAI_USER_ID,
});
```

### Azure Configuration
```javascript
import { AzureTTS } from "@mastra/speech-azure";

const tts = new AzureTTS({
  model: {
    name: "en-US-JennyNeural",
    apiKey: process.env.AZURE_API_KEY,
    region: process.env.AZURE_REGION,
  },
});
```

### Deepgram Configuration
```javascript
import { DeepgramTTS } from "@mastra/speech-deepgram";

const tts = new DeepgramTTS({
  model: {
    name: "aura",
    voice: "asteria-en",
    apiKey: process.env.DEEPGRAM_API_KEY,
  },
});
```

### Google Configuration
```javascript
const tts = new GoogleTTS({
  model: {
    name: "en-US-Standard-A",
    credentials: process.env.GOOGLE_CREDENTIALS,
  },
});
```

### IBM Configuration
```javascript
const tts = new IbmTTS({
  model: {
    voice: "en-US_AllisonV3Voice",
    apiKey: process.env.IBM_API_KEY,
  },
});
```

### Murf Configuration
```javascript
const tts = new MurfTTS({
  model: {
    name: "GEN2",
    voice: "en-US-natalie",
    apiKey: process.env.MURF_API_KEY,
  },
});
```

**Last updated on February 20, 2025**

----------------------------------------
Tts/Stream
https://mastra.ai/docs/reference/tts/stream
----------------------------------------

# TTS API Documentation Summary

## Method: `stream()`

The `stream()` method interacts with the TTS model to produce an audio response stream.

### Parameters
- **text**: `string`  
  The messages to be processed by TTS.
  
- **voice**: `string`  
  Voice ID used for audio generation.

### Returns
- **audioResult**: `Readable`  
  The generated audio stream.

## Example Implementations

### ElevenLabs Streaming
```javascript
import { ElevenLabsTTS } from "@mastra/speech-elevenlabs";

const tts = new ElevenLabsTTS({
  model: {
    name: "eleven_multilingual_v2",
    apiKey: process.env.ELEVENLABS_API_KEY,
  },
});

const voices = await tts.voices();
const voiceId = voices?.[0]?.voice_id;

const { audioResult } = await tts.stream({
  text: "What is AI?",
  voice: voiceId,
});

// Create a write stream for real-time playback
const outputPath = path.join(process.cwd(), "/test-outputs/streaming-output.mp3");
const writeStream = createWriteStream(outputPath);

let totalChunks = 0;
for await (const chunk of audioResult) {
  writeStream.write(chunk);
  console.log(`Received chunk ${++totalChunks}`);
}

writeStream.end();
```

### OpenAI Streaming
```javascript
import { OpenAITTS } from "@mastra/speech-openai";

const tts = new OpenAITTS({
  model: {
    name: "tts-1",
    apiKey: process.env.OPENAI_API_KEY,
  },
});

const voices = await tts.voices();
const voiceId = voices?.[0]?.voice_id;

const { audioResult } = await tts.stream({
  text: "What is AI?",
  voice: voiceId,
});

// Create a write stream for real-time playback
const outputPath = path.join(process.cwd(), "/test-outputs/streaming-output.mp3");
const writeStream = createWriteStream(outputPath);

let totalChunks = 0;
for await (const chunk of audioResult) {
  writeStream.write(chunk);
  console.log(`Received chunk ${++totalChunks}`);
}

writeStream.end();
```

### PlayAI Streaming
```javascript
import { PlayAITTS } from "@mastra/speech-playai";

const tts = new PlayAITTS({
  model: {
    name: "PlayDialog",
    apiKey: process.env.PLAYAI_API_KEY,
  },
  userId: process.env.PLAYAI_USER_ID,
});

const voices = await tts.voices();
const voiceId = voices?.[0]?.voice_id;

const { audioResult } = await tts.stream({
  text: "What is AI?",
  voice: voiceId,
});

// Create a write stream for real-time playback
const outputPath = path.join(process.cwd(), "/test-outputs/streaming-output.mp3");
const writeStream = createWriteStream(outputPath);

let totalChunks = 0;
for await (const chunk of audioResult) {
  writeStream.write(chunk);
  console.log(`Received chunk ${++totalChunks}`);
}

writeStream.end();
```

### Azure Streaming
```javascript
import { AzureTTS } from "@mastra/speech-azure";

const tts = new AzureTTS({
  model: {
    name: "en-US-JennyNeural",
    apiKey: process.env.AZURE_API_KEY,
    region: process.env.AZURE_REGION,
  },
});

const { audioResult } = await tts.stream({ text: "What is AI?" });

// Create a write stream
const outputPath = path.join(process.cwd(), "/test-outputs/azure-stream.mp3");
const writeStream = createWriteStream(outputPath);

// Pipe the audio stream to the file
audioResult.pipe(writeStream);
```

### Deepgram Streaming
```javascript
import { DeepgramTTS } from "@mastra/speech-deepgram";

const tts = new DeepgramTTS({
  model: {
    name: "aura",
    voice: "asteria-en",
    apiKey: process.env.DEEPGRAM_API_KEY,
  },
});

const { audioResult } = await tts.stream({ text: "What is AI?" });

// Create a write stream
const outputPath = path.join(process.cwd(), "/test-outputs/deepgram-stream.mp3");
const writeStream = createWriteStream(outputPath);

// Pipe the audio stream to the file
audioResult.pipe(writeStream);
```

### Google Streaming
```javascript
import { GoogleTTS } from "@mastra/speech-google";

const tts = new GoogleTTS({
  model: {
    name: "en-US-Standard-A",
    credentials: process.env.GOOGLE_CREDENTIALS,
  },
});

const { audioResult } = await tts.stream({ text: "What is AI?" });

// Create a write stream
const outputPath = path.join(process.cwd(), "/test-outputs/google-stream.mp3");
const writeStream = createWriteStream(outputPath);

// Pipe the audio stream to the file
audioResult.pipe(writeStream);
```

### IBM Streaming
```javascript
import { IbmTTS } from "@mastra/speech-ibm";

const tts = new IbmTTS({
  model: {
    voice: "en-US_AllisonV3Voice",
    apiKey: process.env.IBM_API_KEY,
  },
});

const { audioResult } = await tts.stream({ text: "What is AI?" });

// Create a write stream
const outputPath = path.join(process.cwd(), "/test-outputs/ibm-stream.mp3");
const writeStream = createWriteStream(outputPath);

// Pipe the audio stream to the file
audioResult.pipe(writeStream);
```

### Murf Streaming
```javascript
import { MurfTTS } from "@mastra/speech-murf";

const tts = new MurfTTS({
  model: {
    name: "GEN2",
    voice: "en-US-natalie",
    apiKey: process.env.MURF_API_KEY,
  },
});

const { audioResult } = await tts.stream({ text: "What is AI?" });

// Create a write stream
const outputPath = path.join(process.cwd(), "/test-outputs/murf-stream.mp3");
const writeStream = createWriteStream(outputPath);

// Pipe the audio stream to the file
audioResult.pipe(writeStream);
```

**Last updated on February 20, 2025.**

----------------------------------------
Workflows/After
https://mastra.ai/docs/reference/workflows/after
----------------------------------------

### .after() Method Overview

The `.after()` method establishes explicit dependencies between workflow steps, allowing for branching and merging paths during workflow execution.

#### Usage

```javascript
workflow
  .step(stepA)
  .then(stepB)
  .after(stepA); // Creates a new branch after stepA completes

workflow
  .step(stepC);
```

#### Parameters

- **stepId**: `string | string[]`
  - Represents the ID(s) of the step(s) that must be completed before the workflow can continue.

#### Returns

- **workflow**: `Workflow`
  - Returns the workflow instance for method chaining.

#### Related Documentation

- Branching Paths Example
- Workflow Class Reference
- Step Reference
- Control Flow Guide

**Last updated on**: February 20, 2025

----------------------------------------
Workflows/Commit
https://mastra.ai/docs/reference/workflows/commit
----------------------------------------

### Workflow.commit() Method

The `.commit()` method finalizes a workflow definition by validating its structure for execution readiness. It checks for the following conditions:

- No circular dependencies between steps.
- All paths must have an endpoint.
- No unreachable steps.
- No duplicate step IDs.
- No variable references to non-existent steps.

#### Usage

```javascript
workflow
  .step(stepA)
  .then(stepB)
  .commit();
```

#### Returns

- **workflow**: Returns the validated workflow instance.

#### Error Handling

To handle validation errors, use a try-catch block:

```javascript
try {
  workflow
    .step(stepA)
    .after(['stepB', 'stepC'])
    .step(stepD)
    .commit();
} catch (error) {
  if (error instanceof ValidationError) {
    console.log(error.type); // Possible values: 'circular_dependency', 'no_terminal_path', 'unreachable_step', 'duplicate_step_id'
    console.log(error.details);
  }
}
```

#### Validation Error Types

- **circular_dependency**: Steps form a circular reference.
- **no_terminal_path**: Path has no endpoint.
- **unreachable_step**: Step cannot be reached from workflow start.
- **duplicate_step_id**: Multiple steps share the same ID.

### Related Documentation

- Branching Paths Example
- Workflow Class Reference
- Step Reference
- Control Flow Guide

_Last updated on February 20, 2025._

----------------------------------------
Workflows/Createrun
https://mastra.ai/docs/reference/workflows/createRun
----------------------------------------

### API Method: `createRun()`

#### Description
The `createRun()` method initializes a new workflow run instance. It generates a unique run ID for tracking purposes and returns a start function that initiates workflow execution when invoked.

#### Key Points
- **Purpose**: To obtain a unique run ID for tracking, logging, or subscribing via the `.watch()` method.
- **Returns**:
  - `runId`: `string` - A unique identifier for tracking the workflow run.
  - `start`: `() => Promise<WorkflowResult>` - A function that begins workflow execution.

#### Usage Example
```javascript
const { runId, start } = workflow.createRun();

const result = await start();
```

#### Error Handling
The `start` function may throw validation errors if the workflow configuration is invalid. Example error handling:
```javascript
try {
    const { runId, start } = workflow.createRun();
    await start({ triggerData: data });
} catch (error) {
    if (error instanceof ValidationError) {
        // Handle validation errors
        console.log(error.type); // Possible values: 'circular_dependency', 'no_terminal_path', 'unreachable_step'
        console.log(error.details);
    }
}
```

#### Related Documentation
- Workflow Class Reference
- Step Class Reference
- See the "Creating a Workflow" example for complete usage.

_Last updated on February 20, 2025._

----------------------------------------
Workflows/Execute
https://mastra.ai/docs/reference/workflows/execute
----------------------------------------

### Workflow.execute() Method

**Description:**
Executes a committed workflow using provided trigger data and returns the results.

**Usage Example:**
```javascript
const workflow = new Workflow({
  name: "my-workflow",
  triggerSchema: z.object({
    inputValue: z.number()
  })
});

workflow.step(stepOne).then(stepTwo).commit();

const result = await workflow.execute({
  triggerData: { inputValue: 42 }
});
```

**Parameters:**
- `options?`: `ExecuteOptions` - Options for workflow execution.
- `triggerData`: `object` - Data to trigger the workflow.

**Returns:**
- `WorkflowResult`: `object` - Results from workflow execution.
  - `runId`: `string` - Unique identifier for the execution.
  - `results`: `Record<string, StepResult>` - Results of each step.
  - `status`: `string` - Status of the workflow execution.

**Additional Examples:**

1. **Execute with Custom Run ID:**
   ```javascript
   const result = await workflow.execute({
     runId: "custom-run-id",
     triggerData: { inputValue: 42 }
   });
   ```

2. **Handle Execution Results:**
   ```javascript
   const { runId, results, status } = await workflow.execute({
     triggerData: { inputValue: 42 }
   });

   if (status === "COMPLETED") {
     console.log("Step results:", results);
   }
   ```

**Related Methods:**
- `Workflow.createRun()`
- `Workflow.commit()`
- `Workflow.start()`
- `Workflow.suspend()`

**Last Updated:** February 20, 2025

----------------------------------------
Workflows/Resume
https://mastra.ai/docs/reference/workflows/resume
----------------------------------------

### Workflow.resume() Method

The `.resume()` method is used to continue the execution of a suspended workflow step. It allows for the optional inclusion of new context data, which will be merged with the existing step results.

#### Usage

```javascript
await workflow.resume({
  runId: "abc-123",
  stepId: "stepTwo",
  context: {
    secondValue: 100
  }
});
```

#### Parameters

- **config**: `object` - Configuration for resuming the workflow.
  - **runId**: `string` - Unique identifier of the workflow run to resume.
  - **stepId**: `string` - ID of the suspended step to resume.
  - **context**: `Record<string, any>` (optional) - New context data to merge with existing step results.

#### Returns

- **Promise<WorkflowResult>**: An object containing the result of the resumed workflow execution.

#### Error Handling

The `.resume()` method may throw the following errors:

```javascript
try {
  await workflow.resume({
    runId,
    stepId: "stepTwo",
    context: newData
  });
} catch (error) {
  if (error.message === "No snapshot found for workflow run") {
    // Handle missing workflow state
  }
  if (error.message === "Failed to parse workflow snapshot") {
    // Handle corrupted workflow state
  }
}
```

### Related Methods

- **.suspend()**
- **.commit()**

_Last updated on February 20, 2025._

----------------------------------------
Workflows/Start
https://mastra.ai/docs/reference/workflows/start
----------------------------------------

### Workflow API Documentation

#### Method: `.start()`

The `start` function initiates the execution of a workflow run. It processes all steps according to the defined workflow order, managing parallel execution, branching logic, and step dependencies.

#### Usage Example

```javascript
const { runId, start } = workflow.createRun();

const result = await start({
  triggerData: { inputValue: 42 }
});
```

#### Parameters

- **config** (optional): `object`
  - Configuration for starting the workflow run.

  - **triggerData**: `Record<string, any>`
    - Initial data that conforms to the workflow's `triggerSchema`.

#### Returns

- **results**: `Record<string, any>`
  - Combined output from all completed workflow steps.

- **status**: `'completed' | 'error' | 'suspended'`
  - Final status of the workflow run.

#### Error Handling

The `start` function may throw various validation errors. Use the following structure to handle errors:

```javascript
try {
  const result = await start({ triggerData: data });
} catch (error) {
  if (error instanceof ValidationError) {
    console.log(error.type); // Possible values: 'circular_dependency', 'no_terminal_path', 'unreachable_step'
    console.log(error.details);
  }
}
```

#### Related References

- Example: Creating a Workflow
- Example: Suspend and Resume
- `createRun` Reference
- Workflow Class Reference
- Step Class Reference

_Last updated on February 20, 2025_

----------------------------------------
Workflows/Step Class
https://mastra.ai/docs/reference/workflows/step-class
----------------------------------------

### Step Class Overview

The **Step** class represents individual units of work within a workflow, encapsulating execution logic, data validation, and input/output handling.

#### Usage Example

```javascript
const processOrder = new Step({
  id: "processOrder",
  inputSchema: z.object({
    orderId: z.string(),
    userId: z.string()
  }),
  outputSchema: z.object({
    status: z.string(),
    orderId: z.string()
  }),
  execute: async ({ context, runId }) => {
    return {
      status: "processed",
      orderId: context.orderId
    };
  }
});
```

#### Constructor Parameters

- **id**: `string` - Unique identifier for the step.
- **inputSchema**: `z.ZodSchema` - Zod schema to validate input data before execution.
- **outputSchema**: `z.ZodSchema` - Zod schema to validate step output data.
- **payload**: `Record<string, any>` - Static data to be merged with variables.
- **execute**: `(params: ExecuteParams) => Promise<any>` - Async function containing step logic.

#### ExecuteParams

- **context**: `StepContext` - Access to workflow context and step results.
- **runId**: `string` - Unique identifier for the current workflow run.
- **suspend**: `() => Promise<void>` - Function to suspend step execution.

#### Related Documentation

- Workflow Reference
- Step Configuration Guide
- Control Flow Guide

_Last updated on February 20, 2025_

----------------------------------------
Workflows/Step Condition
https://mastra.ai/docs/reference/workflows/step-condition
----------------------------------------

# StepCondition Documentation Summary

## Overview
StepConditions determine the execution of a workflow step based on the output of previous steps or trigger data. Conditions can be specified in three ways: Function Condition, Query Object, and Simple Path Comparison.

## Condition Types

1. **Function Condition**
   - Uses an asynchronous function to evaluate conditions.
   - Example:
     ```javascript
     workflow.step(processOrder, {
       when: async ({ context }) => {
         const auth = context?.getStepPayload<{ status: string }>("auth");
         return auth?.status === "authenticated";
       }
     });
     ```

2. **Query Object**
   - Utilizes a reference to another step's output with a MongoDB-style query.
   - Example:
     ```javascript
     workflow.step(processOrder, {
       when: {
         ref: { step: 'auth', path: 'status' },
         query: { $eq: 'authenticated' }
       }
     });
     ```

3. **Simple Path Comparison**
   - Directly compares the output value using a dot notation path.
   - Example:
     ```javascript
     workflow.step(processOrder, {
       when: {
         "auth.status": "authenticated"
       }
     });
     ```

## StepCondition Structure

- **ref**: 
  - Type: `{ stepId: string | 'trigger'; path: string }`
  - Description: References the output value from a step. `stepId` can be a specific step ID or 'trigger' for initial data. `path` specifies the location in the step result.

- **query**: 
  - Type: `Query<any>`
  - Description: A MongoDB-style query using operators for value comparison.

## Query Operators
The Query object supports the following operators:

- **Comparison Operators**:
  - `$eq`: Equal to value
  - `$ne`: Not equal to value
  - `$gt`: Greater than value
  - `$gte`: Greater than or equal to value
  - `$lt`: Less than value
  - `$lte`: Less than or equal to value

- **Array Operators**:
  - `$in`: Value exists in array
  - `$nin`: Value does not exist in array

- **Logical Operators**:
  - `and`: Array of conditions that must all be true
  - `or`: Array of conditions where at least one must be true

## Related Documentation
- Step Options Reference
- Step Function Reference
- Control Flow Guide

_Last updated on February 20, 2025_

----------------------------------------
Workflows/Step Function
https://mastra.ai/docs/reference/workflows/step-function
----------------------------------------

### Workflow.step() Method

The `.step()` method is used to add a new step to a workflow, allowing for optional configuration of its variables and execution conditions.

#### Usage

```javascript
workflow.step({
  id: "stepTwo",
  outputSchema: z.object({
    result: z.number()
  }),
  execute: async ({ context }) => {
    return { result: 42 };
  }
});
```

#### Parameters

- **stepConfig**: `Step | StepDefinition | string`
  - Represents a Step instance, configuration object, or step ID to be added to the workflow.
  
- **options** (optional): `StepOptions`
  - Additional configuration for step execution.

#### StepDefinition

- **id**: `string`
  - Unique identifier for the step.
  
- **outputSchema** (optional): `z.ZodSchema`
  - Schema for validating the output of the step.
  
- **execute**: `(params: ExecuteParams) => Promise<any>`
  - Function containing the logic for the step.

#### StepOptions

- **variables** (optional): `Record<string, VariableRef>`
  - A map of variable names to their source references.
  
- **when** (optional): `StepCondition`
  - A condition that must be satisfied for the step to execute.

### Related References

- Basic Usage with Step Instance
- Step Class Reference
- Workflow Class Reference
- Control Flow Guide

**Last updated on:** February 20, 2025

----------------------------------------
Workflows/Step Options
https://mastra.ai/docs/reference/workflows/step-options
----------------------------------------

### StepOptions Configuration for Workflows

**Overview:**
`StepOptions` provides configuration options for workflow steps, allowing control over variable mapping, execution conditions, and runtime behavior.

**Usage Example:**

```javascript
workflow.step(processOrder, {
  variables: {
    orderId: { 
      step: 'trigger', 
      path: 'id' 
    },
    userId: { 
      step: 'auth', 
      path: 'user.id' 
    }
  },
  when: {
    ref: { 
      step: 'auth', 
      path: 'status' 
    },
    query: { 
      $eq: 'authenticated' 
    }
  }
});
```

### Properties:

- **variables**: `Record<string, VariableRef>`
  - Maps input variables to values from other steps.

- **when**: `StepCondition`
  - Specifies conditions that must be met for the step to execute.

### VariableRef Structure:

- **step**: `string | Step | { id: string }`
  - Indicates the source step for the variable value.

- **path**: `string`
  - Specifies the path to the value in the step's output.

### Related References:

- Path Comparison
- Step Function Reference
- Step Class Reference
- Workflow Class Reference
- Control Flow Guide

**Last Updated:** February 20, 2025

----------------------------------------
Workflows/Suspend
https://mastra.ai/docs/reference/workflows/suspend
----------------------------------------

### API Method: `.suspend()`

**Description**:  
Pauses workflow execution at the current step until explicitly resumed. The workflow state is persisted and can be continued later.

**Returns**:  
- `Promise<void>`: Resolves when the workflow is successfully suspended.

**Parameters**:
- `metadata?`: `Record<string, any>` (Optional) - Data to store with the suspended state.

### Usage Example:

```javascript
const approvalStep = new Step({
  id: "needsApproval",
  execute: async ({ context, suspend }) => {
    if (context.steps.amount > 1000) {
      await suspend();
    }
    return { approved: true };
  }
});
```

### Additional Examples:

1. **Suspend with Metadata**:

```javascript
const reviewStep = new Step({
  id: "review",
  execute: async ({ context, suspend }) => {
    await suspend({
      reason: "Needs manager approval",
      requestedBy: context.user
    });
    return { reviewed: true };
  }
});
```

2. **Monitor Suspended State**:

```javascript
workflow.watch((state) => {
  if (state.status === "SUSPENDED") {
    notifyReviewers(state.metadata);
  }
});
```

### Related Methods:
- `.resume()`
- `.watch()`
- `.execute()`

**Last updated on**: February 20, 2025

----------------------------------------
Workflows/Then
https://mastra.ai/docs/reference/workflows/then
----------------------------------------

### `.then()` Method Overview

The `.then()` method establishes a sequential dependency between workflow steps, ensuring they execute in a specified order.

#### Usage

```javascript
workflow
  .step(stepOne)
  .then(stepTwo)
  .then(stepThree);
```

#### Parameters

- **step**: `Step | string`  
  The step instance or step ID that should execute after the previous step completes.

#### Returns

- **workflow**: `Workflow`  
  The workflow instance for method chaining.

#### Validation Rules

- The previous step must exist in the workflow.
- Steps cannot form circular dependencies.
- Each step can only appear once in a sequential chain.

#### Error Handling Example

```javascript
try {
  workflow
    .step(stepA)
    .then(stepB)
    .then(stepA) // Will throw error - circular dependency
    .commit();
} catch (error) {
  if (error instanceof ValidationError) {
    console.log(error.type); // 'circular_dependency'
    console.log(error.details);
  }
}
```

### Related References

- Step Reference
- After Reference
- Sequential Steps Example
- Control Flow Guide

**Last updated on February 20, 2025.**

----------------------------------------
Workflows/Watch
https://mastra.ai/docs/reference/workflows/watch
----------------------------------------

### .watch() Method Overview

The `.watch()` function allows you to subscribe to state changes in a Mastra workflow, enabling monitoring of execution progress and reactions to state updates.

#### Usage Example

```javascript
import { Workflow } from "@mastra/core/workflows";

const workflow = new Workflow({
  name: "document-processor"
});

// Subscribe to state changes
const unsubscribe = workflow.watch((state) => {
  console.log('Current step:', state.currentStep);
  console.log('Step outputs:', state.stepOutputs);
});

// Run the workflow
await workflow.run({
  input: { text: "Process this document" }
});

// Stop watching
unsubscribe();
```

#### Parameters

- **callback**: `(state: WorkflowState) => void`
  - A function called whenever the workflow state changes.

#### WorkflowState Properties

- **currentStep**: `string`
  - ID of the currently executing step.
  
- **stepOutputs**: `Record<string, any>`
  - Outputs from completed workflow steps.
  
- **status**: `'running' | 'completed' | 'failed'`
  - Current status of the workflow.
  
- **error**: `Error | null` (optional)
  - Error object if the workflow failed.

#### Returns

- **unsubscribe**: `() => void`
  - Function to stop watching workflow state changes.

#### Additional Examples

1. **Monitor Specific Step Completion**:
   ```javascript
   workflow.watch((state) => {
     if (state.currentStep === 'processDocument') {
       console.log('Document processing output:', state.stepOutputs.processDocument);
     }
   });
   ```

2. **Error Handling**:
   ```javascript
   workflow.watch((state) => {
     if (state.status === 'failed') {
       console.error('Workflow failed:', state.error);
       // Implement error recovery logic
     }
   });
   ```

### Related Topics

- Workflow Creation
- Step Configuration

_Last updated on February 20, 2025._

----------------------------------------
Workflows/Workflow
https://mastra.ai/docs/reference/workflows/workflow
----------------------------------------

# Workflow Class Documentation

The **Workflow** class allows the creation of state machines for complex operations with conditional branching and data validation.

## Import Statement
```javascript
import { Workflow } from "@mastra/core/workflows";
```

## Constructor
```javascript
const workflow = new Workflow({
  name: "my-workflow",
  logger?: Logger<WorkflowLogMessage>,
  steps: Step[],
  triggerSchema?: z.Schema
});
```
### Parameters:
- **name**: `string` - Identifier for the workflow.
- **logger**: `Logger<WorkflowLogMessage>` (optional) - Logger instance for workflow execution details.
- **steps**: `Step[]` - Array of steps included in the workflow.
- **triggerSchema**: `z.Schema` (optional) - Schema for validating workflow trigger data.

## Core Methods
- **step()**
  - Adds a `Step` to the workflow with transitions. Returns the workflow instance for chaining.
  
- **commit()**
  - Validates and finalizes the workflow configuration. Must be called after adding all steps.

- **execute()**
  - Executes the workflow with optional trigger data, typed based on the trigger schema.

## Trigger Schemas
Trigger schemas validate initial data passed to a workflow using Zod.
```javascript
const workflow = new Workflow({
  name: "order-process",
  triggerSchema: z.object({
    orderId: z.string(),
    customer: z.object({
      id: z.string(),
      email: z.string().email(),
    }),
  }),
});
```

### Validation
Validation occurs at two key times:
1. **At Commit Time**: Validates workflow structure, circular dependencies, terminal paths, unreachable steps, variable references, and duplicate step IDs.
2. **During Execution**: Validates trigger data against the schema and checks each step’s input data.

## Workflow Status
Possible status values:
- **CREATED**: Workflow instance created but not started.
- **RUNNING**: Workflow actively executing steps.
- **SUSPENDED**: Execution paused, waiting for resume.
- **COMPLETED**: All steps finished successfully.
- **FAILED**: Encountered an error during execution.

### Example: Handling Different Statuses
```javascript
const { runId, start } = workflow.createRun();

workflow.watch(runId, async ({ status }) => {
  switch (status) {
    case "SUSPENDED":
      // Handle suspended state
      break;
    case "COMPLETED":
      // Process results
      break;
    case "FAILED":
      // Handle error state
      break;
  }
});

await start({ triggerData: data });
```

## Error Handling
```javascript
try {
  const { runId, start } = workflow.createRun();
  await start({ triggerData: data });
} catch (error) {
  if (error instanceof ValidationError) {
    console.log(error.type); // 'circular_dependency' | 'no_terminal_path' | 'unreachable_step'
    console.log(error.details); // { stepId?: string, path?: string[] }
  }
}
```

## Passing Context Between Steps
Steps can access data from previous steps through the context object.
```javascript
workflow
  .step({
    id: 'getData',
    execute: async ({ context }) => {
      return { data: { id: '123', value: 'example' } };
    }
  })
  .step({
    id: 'processData',
    execute: async ({ context }) => {
      const previousData = context.steps.getData.output.data;
      // Process previousData.id and previousData.value
    }
  });
```

### Context Object
- Contains results from all completed steps in `context.steps`.
- Provides access to step outputs through `context.steps.[stepId].output`.
- Is typed based on step output schemas and is immutable for data consistency.

## Related Documentation
- Step
- .then()
- .step()
- .after()

_Last updated on February 20, 2025_

----------------------------------------
Observability/Providers/Braintrust
https://mastra.ai/docs/reference/observability/providers/braintrust
----------------------------------------

# Braintrust Documentation Summary

## Overview
Braintrust is an evaluation and monitoring platform designed for Large Language Model (LLM) applications.

## Configuration
To integrate Braintrust with Mastra, set the following environment variables:

- **OTEL_EXPORTER_OTLP_ENDPOINT**: 
  ```
  https://api.braintrust.dev/otel
  ```

- **OTEL_EXPORTER_OTLP_HEADERS**: 
  ```
  "Authorization=Bearer <Your API Key>, x-bt-parent=project_id:<Your Project ID>"
  ```

## Implementation
To configure Mastra to utilize Braintrust, use the following code snippet:

```javascript
import { Mastra } from "@mastra/core";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "otlp",
    },
  },
});
```

## Dashboard
Access your Braintrust dashboard at: [braintrust.dev](https://braintrust.dev)

**Last updated on**: February 20, 2025

----------------------------------------
Observability/Providers/Laminar
https://mastra.ai/docs/reference/observability/providers/laminar
----------------------------------------

### Laminar Overview
Laminar is an observability platform tailored for Large Language Model (LLM) applications.

### Configuration
To integrate Laminar with Mastra, set the following environment variables:

- **OTEL_EXPORTER_OTLP_ENDPOINT**:  
  `https://api.laminar.dev/v1/traces`

- **OTEL_EXPORTER_OTLP_HEADERS**:  
  `"Authorization=Bearer your_api_key, x-laminar-team-id=your_team_id"`

### Implementation
To configure Mastra for Laminar, use the following code snippet:

```javascript
import { Mastra } from "@mastra/core";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "otlp",
    },
  },
});
```

### Dashboard Access
Access your Laminar dashboard at:  
[https://lmnr.ai/](https://lmnr.ai/)

### Last Updated
February 20, 2025

----------------------------------------
Observability/Providers/Langfuse
https://mastra.ai/docs/reference/observability/providers/langfuse
----------------------------------------

# Langfuse Documentation Summary

## Overview
Langfuse is an open-source observability platform tailored for Large Language Model (LLM) applications.

## Configuration
To integrate Langfuse with Mastra, set the following environment variables:

- `LANGFUSE_PUBLIC_KEY`: Your public key.
- `LANGFUSE_SECRET_KEY`: Your secret key.
- `LANGFUSE_BASEURL`: (Optional) Defaults to `https://cloud.langfuse.com`.

**Important**: Set the `traceName` parameter to `"ai"` for proper integration.

## Implementation
To configure Mastra with Langfuse, use the following code:

```javascript
import { Mastra } from "@mastra/core";
import { LangfuseExporter } from "langfuse-vercel";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "custom",
      traceName: "ai",
      exporter: new LangfuseExporter({
        publicKey: process.env.LANGFUSE_PUBLIC_KEY,
        secretKey: process.env.LANGFUSE_SECRET_KEY,
        baseUrl: process.env.LANGFUSE_BASEURL,
      }),
    },
  },
});
```

## Dashboard
After configuration, access your traces and analytics at the Langfuse dashboard: [cloud.langfuse.com](https://cloud.langfuse.com).

_Last updated on February 20, 2025_

----------------------------------------
Observability/Providers/Langsmith
https://mastra.ai/docs/reference/observability/providers/langsmith
----------------------------------------

# LangSmith Documentation Summary

## Overview
LangSmith is a platform integrated with LangChain for debugging, testing, evaluating, and monitoring Large Language Model (LLM) applications.

## Configuration
To integrate LangSmith with Mastra, configure the following environment variables:

- **OTEL_EXPORTER_OTLP_ENDPOINT**: 
  ```
  https://api.smith.langchain.com/v1/traces
  ```

- **OTEL_EXPORTER_OTLP_HEADERS**: 
  ```
  "Authorization=Bearer your_api_key, x-langsmith-project-id=your_project_id"
  ```

## Implementation
To set up Mastra with LangSmith, use the following code snippet:

```javascript
import { Mastra } from "@mastra/core";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "otlp",
    },
  },
});
```

## Dashboard
Access your traces and analytics through the LangSmith dashboard at: 
```
smith.langchain.com
```

**Last updated on:** February 20, 2025

----------------------------------------
Observability/Providers/Langwatch
https://mastra.ai/docs/reference/observability/providers/langwatch
----------------------------------------

# LangWatch Documentation Summary

## Overview
LangWatch is an observability platform designed for Large Language Model (LLM) applications.

## Configuration
To integrate LangWatch with Mastra, set the following environment variables:

- **LANGWATCH_API_KEY**: Your API key for LangWatch.
- **LANGWATCH_PROJECT_ID**: Your project ID in LangWatch.

## Implementation
To configure Mastra with LangWatch, use the following code snippet:

```javascript
import { Mastra } from "@mastra/core";
import { LangWatchExporter } from "langwatch";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "custom",
      exporter: new LangWatchExporter({
        apiKey: process.env.LANGWATCH_API_KEY,
        projectId: process.env.LANGWATCH_PROJECT_ID,
      }),
    },
  },
});
```

## Dashboard
Access the LangWatch dashboard at: [app.langwatch.ai](http://app.langwatch.ai)

_Last updated on February 20, 2025._

----------------------------------------
Observability/Providers/New Relic
https://mastra.ai/docs/reference/observability/providers/new-relic
----------------------------------------

### New Relic Integration with Mastra

**Overview:**
New Relic is an observability platform that supports OpenTelemetry (OTLP) for full-stack monitoring.

**Configuration:**
To integrate New Relic with Mastra using OTLP, set the following environment variables:

- **OTEL_EXPORTER_OTLP_ENDPOINT**: `https://otlp.nr-data.net:4317`
- **OTEL_EXPORTER_OTLP_HEADERS**: `"api-key=your_license_key"`

**Implementation:**
To configure Mastra for New Relic, use the following code snippet:

```javascript
import { Mastra } from "@mastra/core";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "otlp",
    },
  },
});
```

**Dashboard:**
Access your telemetry data in the New Relic One dashboard at: [one.newrelic.com](https://one.newrelic.com)

**Last Updated:** February 20, 2025

----------------------------------------
Observability/Providers/Signoz
https://mastra.ai/docs/reference/observability/providers/signoz
----------------------------------------

### SigNoz Overview
SigNoz is an open-source Application Performance Monitoring (APM) and observability platform that enables full-stack monitoring using OpenTelemetry.

### Configuration for Mastra
To integrate SigNoz with Mastra, set the following environment variables:

- **OTEL_EXPORTER_OTLP_ENDPOINT**: 
  ```
  https://ingest.{region}.signoz.cloud:443
  ```

- **OTEL_EXPORTER_OTLP_HEADERS**: 
  ```
  signoz-ingestion-key=your_signoz_token
  ```

### Implementation in Mastra
To configure Mastra to utilize SigNoz, use the following code snippet:

```javascript
import { Mastra } from "@mastra/core";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "otlp",
    },
  },
});
```

### Accessing the Dashboard
You can access your SigNoz dashboard at: 
```
cloud.signoz.io
```

**Last updated on**: February 20, 2025

----------------------------------------
Observability/Providers/Traceloop
https://mastra.ai/docs/reference/observability/providers/traceloop
----------------------------------------

# Traceloop Documentation Summary

## Overview
Traceloop is an OpenTelemetry-native observability platform tailored for Large Language Model (LLM) applications.

## Configuration
To integrate Traceloop with Mastra, set the following environment variables:

- **OTEL_EXPORTER_OTLP_ENDPOINT**: 
  ```
  https://api.traceloop.com/v1/traces
  ```

- **OTEL_EXPORTER_OTLP_HEADERS**: 
  ```
  "Authorization=Bearer your_api_key, x-traceloop-destination-id=your_destination_id"
  ```

## Implementation
To configure Mastra to utilize Traceloop, use the following code snippet:

```javascript
import { Mastra } from "@mastra/core";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "otlp",
    },
  },
});
```

## Dashboard
Access your traces and analytics via the Traceloop dashboard at: 
```
app.traceloop.com
```

**Last updated on**: February 20, 2025